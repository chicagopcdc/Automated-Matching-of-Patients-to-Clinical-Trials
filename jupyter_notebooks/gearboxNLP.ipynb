{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Sam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/Sam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Sam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "import docx\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from gensim.models.fasttext import FastText\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object blueprint\n",
    "class gearboxNLP():\n",
    "\n",
    "    \"\"\"\n",
    "    A Python class for automated matching of patients to clinical trials.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -------------------\n",
    "    embedding_model : gensim model object\n",
    "        - loaded upon instantiation after user specification of model path (string)\n",
    "\n",
    "    classifer_model : string\n",
    "        - user-specified filepath (string) for folder containing multiple binary classifiers\n",
    "        - classifiers are sklearn model objects stored as .joblib files\n",
    "        - classifiers are loaded sequentially when ClassifyCriteria() method is called\n",
    "        \n",
    "    trial_info : dictionary\n",
    "        - empty upon initialization\n",
    "        - will contain dictionary of trial information dictionaries with trial ID\n",
    "        as keys\n",
    "        - in addition to trial ID, age, and condition, contains full free-text eligibility\n",
    "        string, raw criteria, cleaned criteria, embeddings, classifications, and match\n",
    "        scores\n",
    "\n",
    "\n",
    "    Methods\n",
    "    -------------------   \n",
    "    GetDocx(filepath):\n",
    "        - accepts a .docx filepath (string) and retrieves full text as string\n",
    "        - used within ExtractTrialInfo() method for full protocol documents\n",
    "        - uses 'python-docx' library\n",
    "        \n",
    "    RepeatRegexFinder(text, regex):\n",
    "        - given a string, returns a list of substrings that intersperse a \n",
    "        given, repeated regular expression\n",
    "        - used within ExtractCriteria() method\n",
    "        \n",
    "    MultiRegexFinder(text, regex_list):\n",
    "        - given a string, returns a list of substrings that intersperse a \n",
    "        given list of unique regular expressions in order of appearance\n",
    "        - used within ExtractCriteria() method\n",
    "        \n",
    "    ExtractTrialInfo(path):\n",
    "        - accepts either an NCT ID or .docx filepath and returns a dictionary\n",
    "        whose contents include the trial ID, age requirements, diagnoses, and\n",
    "        free-text eligibility criteria\n",
    "        - if an NCT ID is passed, the information is automatically retrieved\n",
    "        from clinicaltrials.gov\n",
    "        - if a .docx filepath is passed, the free text is automatically retrieved\n",
    "        and other dictionary contents are left blank until ComputeMatchScore()\n",
    "        method is called\n",
    "        - used within Match() method\n",
    "        \n",
    "    ExtractCriteria(text, mode):\n",
    "        - given eligibility criteria free text, returns a list of discretized eligibility\n",
    "        criteria based on the text format detected\n",
    "        - for text from clinicaltrials.gov, mode = 'ctgov'\n",
    "        - for text from .docx trial protocols, mode = 'docx'\n",
    "        - used within Match() method\n",
    "        \n",
    "    pos_tagger(nltk_tag):\n",
    "        - given a part-of-speech tag from nltk library, return wordnet tag\n",
    "        - used within CleanCriteria() method for lemmatization of text\n",
    "        \n",
    "    CleanCriteria(ExtractedCriteria):\n",
    "        - given list of raw criteria created by ExtractCriteria(), return list of\n",
    "        cleaned and pre-processed text for NLP (lowercasing, removal of single letter\n",
    "        words, removal of various special characters, strip leading and ending whitespace,\n",
    "        lemmatization, etc.)\n",
    "        \n",
    "    sent_vectorizer(sent, model):\n",
    "        - given a sentence and embedding model (specified as attribute above),\n",
    "        returns a sentence embedding using centroid method\n",
    "        - used within EmbedCriteria() method\n",
    "        \n",
    "    EmbedCriteria(CleanedCriteria):\n",
    "        - given list of cleaned criteria created by CleanCriteria(), return a pandas\n",
    "        dataframe of sentence embeddings with their original sentence using the \n",
    "        embedding_model loaded at instantiation\n",
    "        \n",
    "    ClassifyCriteria(criteria, embeddings, model_folder_path):\n",
    "        - given embeddings created by EmbedCriteria() and SVM model path (specified as\n",
    "        attribute above), return pandas dataframe of labeled criteria alongside original\n",
    "        criterion\n",
    "        \n",
    "    ComputeMatchScore(patient, ExtractedCriteria, trialinfo, classified_df):\n",
    "        - given dictionary of patient info, list from ExtractCriteria, trialinfo dict,\n",
    "        and dataframe from ClassifyCriteria(), returns a match score\n",
    "        - match score = (% of matches out out potential matches) * (% of criteria not classified\n",
    "        as \"Other\")\n",
    "        \n",
    "    Match(patient, docx_trials, ctgov_trials):\n",
    "        - given dictionary of patient info, list of filepaths for protocol word documents, and list\n",
    "        of NCT ID's for trials found on clinicaltrials.gov, returns a ranked list of trials, sorted by\n",
    "        highest likelihood of match (as a pandas dataframe)\n",
    "        \n",
    "    \"\"\"    \n",
    "        \n",
    "    #initialization and definition of attributes\n",
    "    def __init__(self, embedding_model, classifier_model):\n",
    "        \n",
    "        #attributes\n",
    "        self.embedding_model = FastText.load(embedding_model)\n",
    "        self.classifier_model = classifier_model\n",
    "        self.trial_info = {}\n",
    "        \n",
    "    #GetDocx() method\n",
    "    def GetDocx(self, filepath):\n",
    "        \n",
    "        #read document using docx\n",
    "        doc = docx.Document(filepath)\n",
    "        \n",
    "        #store paragraphs in list\n",
    "        fullText = []\n",
    "        for para in doc.paragraphs:\n",
    "            fullText.append(para.text)\n",
    "        \n",
    "        #join paragraphs into single string\n",
    "        return '\\n'.join(fullText)\n",
    "\n",
    "    #define function to grab fulltext from word doc or ctgov\n",
    "    def ExtractTrialInfo(self, path):\n",
    "\n",
    "        #create list of desired variables to extract from xml\n",
    "        variables = [\"NCT_id\", \"condition\", \"./eligibility/minimum_age\", \"./eligibility/maximum_age\", \"./eligibility/criteria/textblock\"]\n",
    "\n",
    "        #create dict in which each key is one of the variables\n",
    "        variables_dict = {}.fromkeys(variables)\n",
    "\n",
    "        #if path ends in .docx\n",
    "        if path.endswith('.docx'):\n",
    "\n",
    "            #assign first 9 letters of path as trial name (e.e. APAL2020D)\n",
    "            variables_dict[\"NCT_id\"] = path.split(\"/\")[-1][:9]\n",
    "\n",
    "            #get text from word doc and add to dict\n",
    "            text = self.GetDocx(path)\n",
    "            variables_dict[\"./eligibility/criteria/textblock\"] = text\n",
    "\n",
    "        #otherwise, NCT ID is being used\n",
    "        else:\n",
    "\n",
    "            #server, trial ID, and ext to return xml for trial\n",
    "            server = \"https://clinicaltrials.gov/ct2/show/\"\n",
    "            trial = path\n",
    "            ext = \"?displayxml=true\"\n",
    "\n",
    "            #make request\n",
    "            response = requests.get(server + trial + ext)\n",
    "\n",
    "            #format response as xml\n",
    "            tree = ET.fromstring(response.content)\n",
    "\n",
    "            #iterate through variables to extract\n",
    "            for variable in variables_dict.keys():\n",
    "                if variable == \"condition\":\n",
    "                    variables_dict[variable] = []\n",
    "                    for each in tree.findall(variable):\n",
    "                        variables_dict[variable].append(each.text)\n",
    "                elif variable == \"NCT_id\":\n",
    "                    variables_dict[variable] = path\n",
    "                else:\n",
    "                    variables_dict[variable] = tree.findtext(variable)        \n",
    "\n",
    "        #add to trial_info attribute and return dict of trial info\n",
    "        self.trial_info[variables_dict['NCT_id']] = variables_dict\n",
    "        return variables_dict\n",
    "    \n",
    "    #RepeatRegexFinder() method\n",
    "    def RepeatRegexFinder(self, text, regex):\n",
    "\n",
    "        #create empty lists in which to store starts and stops\n",
    "        starts = []\n",
    "        stops = []\n",
    "\n",
    "        #iterate over text to find indices matching regex and its start and stops\n",
    "        for match in re.finditer(regex, text):\n",
    "\n",
    "            #note start and stop of matching sequence\n",
    "            starts.append(match.span()[0])\n",
    "            stops.append(match.span()[1])\n",
    "\n",
    "        #create empty list in which to store subcontents\n",
    "        subcontents = []\n",
    "\n",
    "        #loop through starts and stops and add desired slices to list--leaving last subcontent off\n",
    "        for x in range(len(starts) - 1):\n",
    "\n",
    "            #store each mainbullet slice\n",
    "            subcontent = text[stops[x]:starts[x + 1]]\n",
    "            subcontents.append(subcontent)\n",
    "\n",
    "        #add last subcontent as last slice from 'text'\n",
    "        lastsubcontent = text[stops[-1]:]\n",
    "        subcontents.append(lastsubcontent)\n",
    "\n",
    "        #return list of subcontents\n",
    "        return subcontents\n",
    "    \n",
    "    #MultiRegexFinder() method\n",
    "    def MultiRegexFinder(self, text, regex_list):\n",
    "\n",
    "        #create empty list in which to store indices for starts of regex's\n",
    "        starts = []\n",
    "\n",
    "        #iterate through regex's\n",
    "        for regex in regex_list:\n",
    "\n",
    "            #compile each regex\n",
    "            re_compiled = re.compile(regex)\n",
    "\n",
    "            #search text for regex\n",
    "            re_search = re_compiled.search(text)\n",
    "\n",
    "            #if search is not empty, add start to above empty list\n",
    "            if re_search != None:\n",
    "                start = re_search.span()[0]\n",
    "                starts.append(start)\n",
    "\n",
    "            #otherwise, do nothing\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #sort starts in case out of order    \n",
    "        starts.sort()\n",
    "\n",
    "        #create empty list in which to store subcontents attached to each regex\n",
    "        subcontents = []\n",
    "\n",
    "        #loop through starts and add desired content slices to list--leaving last bullet off\n",
    "        for x in range(len(starts) - 1):\n",
    "\n",
    "            #add individual content slice of text to above list\n",
    "            subcontent = text[starts[x]:starts[x + 1]]\n",
    "            subcontents.append(subcontent)\n",
    "\n",
    "        #add last header content slice to list\n",
    "        lastsubcontent = text[starts[-1]:]\n",
    "        subcontents.append(lastsubcontent)\n",
    "\n",
    "        #return subcontents\n",
    "        return subcontents\n",
    "    \n",
    "    #define ExtractCriteria()\n",
    "    def ExtractCriteria(self, text, mode):\n",
    "\n",
    "        #parse text from clinicaltrials.gov\n",
    "        if mode == 'ctgov':\n",
    "\n",
    "            #empty output_list to store criteria in\n",
    "            output_list = []\n",
    "\n",
    "            #create lists of headers for later use in format detection\n",
    "            majorheaders = [r'DISEASE CHARACTERISTICS', r'PATIENT CHARACTERISTICS', r'PRIOR CONCURRENT THERAPY', r'DONOR CHARACTERISTICS']\n",
    "            patientchars = [r'Age', r'Performance status', r'Life expectancy', r'Hematopoietic', r'Hepatic', r'Renal', r'Cardiovascular', r'Pulmonary', r'Other']\n",
    "            priortherapy = [r'Biologic therapy', r'Chemotherapy', r'Endocrine therapy', r'Radiotherapy', r'Surgery', r'Other']\n",
    "\n",
    "            #extraction for: Inclusion and Exclusion Criteria/No Nested Subbullets\n",
    "            #if inclusion/exclusion criteria followed by any multiple charcters then ':', AND no subbullets\n",
    "            if ((re.search(r\"inclusion criteria(.*):\", text.lower()) != None) or (re.search(r\"exclusion criteria(.*):\", text.lower()) != None)) and (re.search(r'\\r\\n\\r\\n {15}\\S', text) == None):\n",
    "\n",
    "                #split text at at each bullet/number, almost always noted by double carriage return, and add to output list\n",
    "                text = text.split(\"\\r\\n\\r\\n\")\n",
    "\n",
    "                #add criteria to output list\n",
    "                for each in text:\n",
    "                    output_list.append(each)      \n",
    "\n",
    "            #extraction for: Inclusion and Exclusion Criteria/Nested Subbullets\n",
    "            #if inclusion/exclusion criteria followed by any multiple charcters then ':', AND contains subbullets\n",
    "            elif ((re.search(r\"inclusion criteria(.*):\", text.lower()) != None) or (re.search(r\"exclusion criteria(.*):\", text.lower()) != None)) and (re.search(r'\\r\\n\\r\\n {15}\\S', text) != None):\n",
    "\n",
    "                #use RepeatRegexFinder() to return list of mainbullets and their subcontents\n",
    "                #mainbullets are denoted by the pattern of double carriage return followed by exactly 10 spaces\n",
    "                mainbullets = self.RepeatRegexFinder(text = text, regex = r'\\r\\n\\r\\n {10}\\S')\n",
    "\n",
    "                #set regex as raw string for subbullets - line break followed by 3 or more spaces, then nonwhitespace character\n",
    "                regex = r'\\r\\n\\r\\n {3,}\\S'\n",
    "\n",
    "                #compile each regex\n",
    "                re_compiled = re.compile(regex)\n",
    "\n",
    "                #add criteria to output list\n",
    "                for each in mainbullets:\n",
    "\n",
    "                    #search text for regex\n",
    "                    re_search = re_compiled.search(each)\n",
    "\n",
    "                    #if the first subbullet regex is found\n",
    "                    if re_search != None:\n",
    "\n",
    "                        #create empty lists in which to store starts of subbullets\n",
    "                        starts = []\n",
    "\n",
    "                        #iterate over text to find indices matching regex and its start\n",
    "                        for match in re.finditer(regex, each):\n",
    "\n",
    "                            #note start and stop of matching sequence\n",
    "                            starts.append(match.span()[0])\n",
    "\n",
    "                        #for each subbullet detected\n",
    "                        for x in range(len(starts) - 1):\n",
    "\n",
    "                            #concatenate subbullet to base_statement and add to new list\n",
    "                            new_criterion = each[starts[x]:starts[x + 1]]\n",
    "                            output_list.append(new_criterion)\n",
    "\n",
    "                    #otherwise, ignore\n",
    "                    else:\n",
    "                        output_list.append(each)\n",
    "\n",
    "            #extraction for: Major Header/Subbullets\n",
    "            #if contains major head and 0 or 1 capitalized subheaders\n",
    "            elif (any(term in text for term in majorheaders) and (np.count_nonzero([term in text for term in patientchars]) < 4)):\n",
    "\n",
    "                #use MultiRegexFinder to return header subcontents\n",
    "                headercontents = self.MultiRegexFinder(text = text, regex_list = majorheaders)\n",
    "\n",
    "                #deal with subbullet handling in the same way used above\n",
    "                #iterate through each header content\n",
    "                for subtext in headercontents:\n",
    "\n",
    "                    #if carriage returns exist\n",
    "                    if re.search(r'\\r\\n\\r\\n {10}\\S', subtext) != None:\n",
    "\n",
    "                        #use RepeatRegexFinder() to return list of mainbullets and their subcontents\n",
    "                        #mainbullets are denoted by the pattern of double carriage return followed by exactly 10 spaces\n",
    "                        subtexts = self.RepeatRegexFinder(text = subtext, regex = r'\\r\\n\\r\\n {10}\\S')\n",
    "\n",
    "                        #set regex as raw string for subbullets - line break followed by 3 or more spaces, then nonwhitespace character\n",
    "                        regex = r'\\r\\n\\r\\n {3,}\\S'\n",
    "\n",
    "                        #compile each regex\n",
    "                        re_compiled = re.compile(regex)\n",
    "\n",
    "                        #add criteria to output list\n",
    "                        for each in subtexts:\n",
    "\n",
    "                            #search text for regex\n",
    "                            re_search = re_compiled.search(each)\n",
    "\n",
    "                            #if the first subbullet regex is found\n",
    "                            if re_search != None:\n",
    "\n",
    "                                #create empty lists in which to store starts of subbullets\n",
    "                                starts = []\n",
    "\n",
    "                                #iterate over text to find indices matching regex and its start\n",
    "                                for match in re.finditer(regex, each):\n",
    "\n",
    "                                    #note start and stop of matching sequence\n",
    "                                    starts.append(match.span()[0])\n",
    "\n",
    "                                #for each subbullet detected\n",
    "                                for x in range(len(starts) - 1):\n",
    "\n",
    "                                    #concatenate subbullet to base_statement and add to new list\n",
    "                                    new_criterion = each[starts[x]:starts[x + 1]]\n",
    "                                    output_list.append(new_criterion)\n",
    "\n",
    "                            #otherwise, ignore\n",
    "                            else:\n",
    "                                output_list.append(each)\n",
    "\n",
    "                    #otherwise has no bullets - split on period, colon, or semicolon followed by at least one space\n",
    "                    else:\n",
    "                        try:\n",
    "                            subtexts = self.RepeatRegexFinder(text = subtext, regex =  r\"[\\.;] {1,}\")\n",
    "                            for each in subtexts:\n",
    "                                output_list.append(each)\n",
    "                        except:\n",
    "                            output_list.append(subtext)\n",
    "\n",
    "            #extraction for last remaining Format ID: Major Header/Subheader/Subbullets\n",
    "            #this process will be the most complex, as it has the most heterogeneity\n",
    "            #if contains major head and >=2 capitalized subheaders\n",
    "            elif (any(term in text for term in majorheaders)) and (np.count_nonzero([term in text for term in patientchars]) >= 4):\n",
    "\n",
    "                #use MultiRegexFinder to return major header subcontents\n",
    "                headercontents = self.MultiRegexFinder(text = text, regex_list = majorheaders)\n",
    "\n",
    "                #deal with subbullet/subheader handling for each major header section\n",
    "                #iterate through each header content        \n",
    "                for subtext in headercontents:\n",
    "\n",
    "                    #if patient characteristics in subtext (regardless of capitalization pattern), handle as list with subheaders/bullets from patientchars\n",
    "                    if \"patient characteristics\" in subtext.lower():\n",
    "\n",
    "                        #use MultiRegexFinder() to return subcontents of patientchars headers\n",
    "                        subcontents = self.MultiRegexFinder(text = subtext, regex_list = patientchars)\n",
    "\n",
    "                        #set regex as raw string for subbullets - line break followed by 3 or more spaces, then nonwhitespace character\n",
    "                        regex = r'\\r\\n\\r\\n {3,}\\S'\n",
    "\n",
    "                        #compile each regex\n",
    "                        re_compiled = re.compile(regex)\n",
    "\n",
    "                        #add to raw criteria\n",
    "                        for each in subcontents:\n",
    "\n",
    "                            #search text for regex\n",
    "                            re_search = re_compiled.search(each)\n",
    "\n",
    "                            #if the first subbullet regex is found\n",
    "                            if re_search != None:\n",
    "\n",
    "                                #create empty lists in which to store starts of subbullets\n",
    "                                starts = []\n",
    "\n",
    "                                #iterate over text to find indices matching regex and its start\n",
    "                                for match in re.finditer(regex, each):\n",
    "\n",
    "                                    #note start and stop of matching sequence\n",
    "                                    starts.append(match.span()[0])\n",
    "\n",
    "                                #for each subbullet detected\n",
    "                                for x in range(len(starts) - 1):\n",
    "\n",
    "                                    #concatenate subbullet to base_statement and add to new list\n",
    "                                    new_criterion = each[starts[x]:starts[x + 1]]\n",
    "                                    output_list.append(new_criterion)\n",
    "\n",
    "                            #otherwise, ignore\n",
    "                            else:\n",
    "                                output_list.append(each)\n",
    "\n",
    "\n",
    "                    #if prior concurrent therapy in subtext (regardless of capitalization pattern), handle as bulleted list with subheaders listed above\n",
    "                    #the \"chemotherapy\" specification was added to ensure this section occurs in subheader format\n",
    "                    #when \"chemotherapy\" is not present, the format does not have subheaders and will be handled otherwise\n",
    "                    elif (\"prior concurrent therapy\" in subtext.lower()) and (\"chemotherapy\" in subtext.lower()):\n",
    "\n",
    "                        #use MultiRegexFinder() to return subcontents of priortherapy headers\n",
    "                        subcontents = self.MultiRegexFinder(text = subtext, regex_list = priortherapy)\n",
    "\n",
    "                        #set regex as raw string for subbullets - line break followed by 3 or more spaces, then nonwhitespace character\n",
    "                        regex = r'\\r\\n\\r\\n {3,}\\S'\n",
    "\n",
    "                        #compile each regex\n",
    "                        re_compiled = re.compile(regex)\n",
    "\n",
    "                        #add to raw criteria\n",
    "                        for each in subcontents:\n",
    "\n",
    "                            #search text for regex\n",
    "                            re_search = re_compiled.search(each)\n",
    "\n",
    "                            #if the first subbullet regex is found\n",
    "                            if re_search != None:\n",
    "\n",
    "                                #create empty lists in which to store starts of subbullets\n",
    "                                starts = []\n",
    "\n",
    "                                #iterate over text to find indices matching regex and its start\n",
    "                                for match in re.finditer(regex, each):\n",
    "\n",
    "                                    #note start and stop of matching sequence\n",
    "                                    starts.append(match.span()[0])\n",
    "\n",
    "                                #for each subbullet detected\n",
    "                                for x in range(len(starts) - 1):\n",
    "\n",
    "                                    #concatenate subbullet to base_statement and add to new list\n",
    "                                    new_criterion = each[starts[x]:starts[x + 1]]\n",
    "                                    output_list.append(new_criterion)\n",
    "\n",
    "                            #otherwise, ignore\n",
    "                            else:\n",
    "                                output_list.append(each)\n",
    "\n",
    "                    #otherwise, most can be treated as bulleted list with potential subbullets as with other format IDs\n",
    "                    elif re.search(r'\\r\\n\\r\\n {10}\\S', subtext) != None:\n",
    "\n",
    "                        #use RepeatRegexFinder() to return list of mainbullets and their subcontents\n",
    "                        #mainbullets are denoted by the pattern of double carriage return followed by exactly 10 spaces\n",
    "                        mainbullets = self.RepeatRegexFinder(text = subtext, regex = r'\\r\\n\\r\\n {10}\\S')\n",
    "\n",
    "                        #set regex as raw string for subbullets - line break followed by 3 or more spaces, then nonwhitespace character\n",
    "                        regex = r'\\r\\n\\r\\n {3,}\\S'\n",
    "\n",
    "                        #compile each regex\n",
    "                        re_compiled = re.compile(regex)\n",
    "\n",
    "                        #add to raw criteria\n",
    "                        for each in mainbullets:\n",
    "\n",
    "                            #search text for regex\n",
    "                            re_search = re_compiled.search(each)\n",
    "\n",
    "                            #if the first subbullet regex is found\n",
    "                            if re_search != None:\n",
    "\n",
    "                                #create empty lists in which to store starts of subbullets\n",
    "                                starts = []\n",
    "\n",
    "                                #iterate over text to find indices matching regex and its start\n",
    "                                for match in re.finditer(regex, each):\n",
    "\n",
    "                                    #note start and stop of matching sequence\n",
    "                                    starts.append(match.span()[0])\n",
    "\n",
    "                                #for each subbullet detected\n",
    "                                for x in range(len(starts) - 1):\n",
    "\n",
    "                                    #concatenate subbullet to base_statement and add to new list\n",
    "                                    new_criterion = each[starts[x]:starts[x + 1]]\n",
    "                                    output_list.append(new_criterion)\n",
    "\n",
    "                            #otherwise, ignore\n",
    "                            else:\n",
    "                                output_list.append(each)\n",
    "\n",
    "                    #otherwise, subtext is a full paragraph without demarcation between individual criteria\n",
    "                    else:\n",
    "                        try:\n",
    "                            subtexts = self.RepeatRegexFinder(text = subtext, regex =  r\"[\\.;] {1,}\")\n",
    "                            for each in subtexts:\n",
    "                                output_list.append(each)\n",
    "                        except:\n",
    "                            output_list.append(subtext)\n",
    "\n",
    "\n",
    "            #otherwise, treat as bulleted list and split bullets on sentences\n",
    "            else:\n",
    "\n",
    "                #split text at at each bullet/number, almost always noted by double carriage return, and add to output list\n",
    "                text = text.split(\"\\r\\n\\r\\n\")\n",
    "\n",
    "                #add criteria to output list\n",
    "                for subtext in text:\n",
    "                    try:\n",
    "                        subtexts = self.RepeatRegexFinder(text = subtext, regex =  r\"[\\.;] {1,}\")\n",
    "                        for each in subtexts:\n",
    "                            output_list.append(each)\n",
    "                    except:\n",
    "                        output_list.append(subtext)\n",
    "\n",
    "            #return separated inclusion/exclusion criteria if possible, otherwise return list\n",
    "            exc_start = 0\n",
    "            for each in output_list:\n",
    "                if \"exclusion criteria\" in each.lower():\n",
    "                    exc_start = output_list.index(each)\n",
    "                else:\n",
    "                    pass\n",
    "            if exc_start != 0:\n",
    "                inclusioncriteria = output_list[:exc_start]\n",
    "                exclusioncriteria = output_list[exc_start:]\n",
    "                return [inclusioncriteria, exclusioncriteria]\n",
    "            else:\n",
    "                return output_list\n",
    "\n",
    "        #parse text from trial protocol word document \n",
    "        elif mode == 'docx':\n",
    "\n",
    "            #define output list to store criteria\n",
    "            output_list = []\n",
    "\n",
    "            #find eligibility criteria text between Eligibility and Arms/Regimens\n",
    "            ec_window = [r'\\n\\nEligibility', \n",
    "                         r'\\n\\nArms/Regimens']\n",
    "            fullcriteria = self.MultiRegexFinder(text = text, regex_list = ec_window)[0]\n",
    "\n",
    "            #define possible subheaders to detect\n",
    "            subheader_windows = [r'\\nAge',\n",
    "                                 r' Age:',\n",
    "                                 r'\\nWeight',\n",
    "                                 r'\\nDiagnosis',\n",
    "                                 r'\\nDisease Status',\n",
    "                                 r'\\nPerformance Status', \n",
    "                                 r'\\nPrior Therapy',\n",
    "                                 r'\\nOrgan function criteria',\n",
    "                                 r'\\nOrgan Function Requirements',\n",
    "                                 r'\\nExclusion Criteria',\n",
    "                                 r'\\nExclusion criteria',\n",
    "                                 r'\\tExclusion Criteria',\n",
    "                                 r'\\nConcomitant Medications', \n",
    "                                 r'\\nPregnancy or Breast-Feeding', \n",
    "                                 r'\\nInfection',\n",
    "                                 r'\\nSystemic Diseases']\n",
    "\n",
    "            #detect subheaders and store in list\n",
    "            splitcriteria = self.MultiRegexFinder(text = fullcriteria, regex_list = subheader_windows)\n",
    "\n",
    "            #make lists to store inclusion/exclusion criteria\n",
    "            inclusioncriteria = []\n",
    "            exclusioncriteria = []\n",
    "\n",
    "            #split criteria on line breaks if many bullets (>1), else split on sentences if many sentences (>1)\n",
    "            for each in splitcriteria:\n",
    "                if each.startswith(\"\\nAge\") or each.startswith(\"Age\"):\n",
    "                    output_list.append(each)\n",
    "                elif each.startswith(\"\\nDiagnosis\") or each.startswith(\"\\nDisease Status\"):\n",
    "                    output_list.append(each)\n",
    "                    for criterion in each.split(\"\\n\"):\n",
    "                        if \"CNS\" in criterion:\n",
    "                            inclusioncriteria.append(criterion)\n",
    "                        else:\n",
    "                            pass\n",
    "                elif \"exclusion criteria\" not in each.lower():\n",
    "                    if len(each.split(\"\\n\")) > 4:\n",
    "                        for subeach in each.split(\"\\n\"):\n",
    "                            inclusioncriteria.append(subeach)\n",
    "                    else:\n",
    "                        regex = r\"[\\.:;] {1,}\"\n",
    "                        if len([x for x in re.finditer(regex, each)]) > 1:\n",
    "                            for subeach in self.RepeatRegexFinder(text = each, regex = regex):\n",
    "                                inclusioncriteria.append(subeach)\n",
    "                        else:\n",
    "                            inclusioncriteria.append(each)\n",
    "                else:\n",
    "                    if len(each.split(\"\\n\")) > 4:\n",
    "                        for subeach in each.split(\"\\n\"):\n",
    "                            exclusioncriteria.append(subeach)\n",
    "                    else:\n",
    "                        regex = r\"[\\.:;] {1,}\"\n",
    "                        if len([x for x in re.finditer(regex, each)]) > 1:\n",
    "                            for subeach in self.RepeatRegexFinder(text = each, regex = regex):\n",
    "                                exclusioncriteria.append(subeach)\n",
    "                        else:\n",
    "                            exclusioncriteria.append(each)\n",
    "\n",
    "            #add to output list\n",
    "            output_list.append(inclusioncriteria)\n",
    "            output_list.append(exclusioncriteria)\n",
    "\n",
    "            #return output list\n",
    "            return output_list\n",
    "\n",
    "        #must specify mode\n",
    "        else:\n",
    "            print(\"You must specify mode as 'ctgov' or 'docx'.\")\n",
    "            \n",
    "    #define pos_tagger()\n",
    "    def pos_tagger(self, nltk_tag): \n",
    "        \n",
    "        #if tagged as J, mark as adjective\n",
    "        if nltk_tag.startswith('J'): \n",
    "            return wordnet.ADJ    \n",
    "        \n",
    "        #if tagged as V, mark as verb\n",
    "        elif nltk_tag.startswith('V'): \n",
    "            return wordnet.VERB \n",
    "\n",
    "        #if tagged as N, mark as noun\n",
    "        elif nltk_tag.startswith('N'): \n",
    "            return wordnet.NOUN \n",
    "        \n",
    "        #if tagged as R, mark as adverb\n",
    "        elif nltk_tag.startswith('R'): \n",
    "            return wordnet.ADV     \n",
    "        \n",
    "        #else, don't mark\n",
    "        else:           \n",
    "            return None\n",
    "        \n",
    "    #define CleanCriteria()\n",
    "    def CleanCriteria(self, ExtractedCriteria):\n",
    "\n",
    "        #create empty list in which to store cleaned text with the following changes\n",
    "        final_criteria = []\n",
    "\n",
    "        #compile regular expression to detect strings of all caps (i.e. abbreviations)\n",
    "        regex = r\"\\b[A-Z]{2,}\\b\"\n",
    "        re_compiled = re.compile(regex)\n",
    "\n",
    "        #list of common all caps non-abbreviation words (appear > 1x)\n",
    "        non_abrv = [\"DONOR\", \"DISEASE\", \"CHARACTERISTICS\", \"AND\", \"DONORS\", \"RELATED\", \"OR\", \"INCLUSION\", \"CRITERIA\", \"EXCLUSION\", \"PRIOR\", \"CONCURRENT\", \"THERAPY\", \"NOTE\", \"BEFORE\", \"PATIENTS\", \"MATCHED\", \"UNRELATED\", \"MUST\", \"REAL\", \"TRANSPLANT\", \"PATIENT\", \"ELIGIBILITY\", \"ALLOWED\", \"ADULT\", \"PEDIATRIC\", \"ORGAN\", \"DYSFUNCTION\", \"EXCEPT\", \"STRATUM\", \"STRATA\", \"GROUP\", \"AGED\"]\n",
    "\n",
    "        #list of custom stop words based on top 100 terms, many removed for semantic significance\n",
    "        custom_stops = ['or','of','the','patients','to','for','with','no','and','at','not','must','be','have','in',\n",
    "                        'are','than','as', 'by','is','study','other','on', 'who','if', 'will','any', 'criteria','patient',\n",
    "                        'from','this','that','allowed','an','may','all','known']\n",
    "\n",
    "        #list of additional suffixes to be removed\n",
    "        suffix_list = [\"tion\", \"ical\", \"ious\", \"ance\"]\n",
    "\n",
    "        #conduct pre-processing steps for each criterion in list\n",
    "        for each in ExtractedCriteria:\n",
    "\n",
    "            #break criterion into single words\n",
    "            word_list = []\n",
    "            for word in each.split():\n",
    "\n",
    "                #search word for abbreviations\n",
    "                re_search = re_compiled.search(word)\n",
    "\n",
    "                #if search is not empty and word isn't a commonly all-caps non abbreviation\n",
    "                #keep abbreviation as is, otherwise lowercase\n",
    "                #get rid of stop words as well\n",
    "                if (re_search != None) & (not any(term in word for term in non_abrv)):\n",
    "                    word_list.append(word) \n",
    "                elif word.lower() not in custom_stops:\n",
    "                    word = word.lower()\n",
    "                    word_list.append(word)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            #reassign \"each\" to sentence that is lowercased except for abbreviations\n",
    "            sentence = \" \".join(word_list)\n",
    "\n",
    "            #remove all special characters except numbers and ';' (often used in genetic mutations)\n",
    "            sentence = re.sub(r'[^A-z0-9 ;]', \"\", sentence)\n",
    "\n",
    "            #remove all single characters\n",
    "            sentence = re.sub(r'\\s+[a-zA-Z0-9]\\s+', \"\", sentence)\n",
    "\n",
    "            #tokenize the sentence and find the POS tag for each token \n",
    "            pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))   \n",
    "\n",
    "            #use previously defined function to fix tags\n",
    "            #reference: https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/#:~:text=Wordnet%20Lemmatizer%20(with%20POS%20tag)&text=This%20is%20because%20these%20words,%2C%20noun%2C%20adjective%20etc).\n",
    "            wordnet_tagged = list(map(lambda x: (x[0], self.pos_tagger(x[1])), pos_tagged)) \n",
    "\n",
    "            #instantiate lemmatizer object to be used with CleanCriteria\n",
    "            lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "            #create empty list in which to store lemmatized sentence\n",
    "            lemmatized_sentence = [] \n",
    "\n",
    "            #iterate through mapped list with wordnet tags\n",
    "            for word, tag in wordnet_tagged: \n",
    "                #if there is no available tag, append the token as is \n",
    "                if tag is None: \n",
    "                    lemmatized_sentence.append(word) \n",
    "\n",
    "                # else use the tag to lemmatize the token         \n",
    "                else:         \n",
    "                    lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
    "\n",
    "            #remove selected suffixes from words that are poorly handled by automatic lemmatizer\n",
    "            for index in range(len(lemmatized_sentence)):\n",
    "                if lemmatized_sentence[index][-4:] in suffix_list:\n",
    "                    lemmatized_sentence[index] = lemmatized_sentence[index][:-4]\n",
    "\n",
    "            #join previously created list into sentence (i.e. single string)\n",
    "            lemmatized_sentence = \" \".join(lemmatized_sentence) \n",
    "\n",
    "            #replace multiple whitespace with single whitespace\n",
    "            lemmatized_sentence = re.sub(\" +\", \" \", lemmatized_sentence)\n",
    "\n",
    "            #strip leading and ending whitespace\n",
    "            lemmatized_sentence = lemmatized_sentence.strip()\n",
    "\n",
    "            #add to new empty list\n",
    "            final_criteria.append(lemmatized_sentence)\n",
    "\n",
    "        #make df with original and final\n",
    "        df = pd.DataFrame({'Original':ExtractedCriteria, 'Final':final_criteria})\n",
    "        df = df[df['Original'] != '']\n",
    "        return df\n",
    "    \n",
    "    #define sent_vectorizer()\n",
    "    def sent_vectorizer(self, sent, model):\n",
    "\n",
    "        #empty list in which to store sentence vectors\n",
    "        sent_vec =[]\n",
    "\n",
    "        #keeps track of total number of words in sentence\n",
    "        numw = 0\n",
    "\n",
    "        #for each word in a sentence\n",
    "        for w in sent:\n",
    "\n",
    "            #if this is the first word, sentence vector starts out with single embedding\n",
    "            #if not the first word, add word embedding to previous embeddings as part of cumulative sentence vector\n",
    "            try:\n",
    "                if numw == 0:\n",
    "                    sent_vec = model.wv[w]\n",
    "                else:\n",
    "                    sent_vec = np.add(sent_vec, model.wv[w])\n",
    "\n",
    "                #add 1 to word counter for each iteration\n",
    "                numw+=1\n",
    "\n",
    "            #if there's an error, do nothing\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        #when finished, return the overall sentence vector divided by the number of words \n",
    "        return np.asarray(sent_vec) / numw\n",
    "\n",
    "    #def EmbedCriteria()\n",
    "    def EmbedCriteria(self, CleanedCriteria):\n",
    "\n",
    "        #set embedding model\n",
    "        ft_model = self.embedding_model\n",
    "\n",
    "        #tokenize criteria\n",
    "        tokenized = [nltk.word_tokenize(criterion) for criterion in CleanedCriteria]\n",
    "\n",
    "        #create empty list in which to store sentence embeddings\n",
    "        X = []\n",
    "\n",
    "        #for each criterion in overall data list, vectorize the sentence and append to X\n",
    "        for sentence in tokenized:\n",
    "            X.append(self.sent_vectorizer(sentence, ft_model))\n",
    "\n",
    "        #make df with final and embedding\n",
    "        df = pd.DataFrame({'Final':CleanedCriteria, 'Embedding':X})\n",
    "        return df\n",
    "\n",
    "    #define ClassifyCriteria()\n",
    "    def ClassifyCriteria(self, criteria, embeddings, model_folder_path):\n",
    "\n",
    "        #create empty dict in which to store various class probabilities from SVM\n",
    "        probabilities = {}\n",
    "\n",
    "        #iterate through models and obtain predictions on input data\n",
    "        for each in os.listdir(model_folder_path):\n",
    "            if each.endswith(\".joblib\"):\n",
    "                model = load(model_folder_path + each)\n",
    "                two_sided_prob = model.predict_proba(list(embeddings))\n",
    "                prob_outcome = two_sided_prob[:,1]\n",
    "                probabilities[each.strip(\".joblib\")] = list(prob_outcome)\n",
    "\n",
    "        #cast probability dict into dataframe\n",
    "        prob_df = pd.DataFrame(probabilities)\n",
    "\n",
    "        #choose most likely class as highest probability across models (i.e. one vs. all)\n",
    "        mostlikelyclass = prob_df.idxmax(axis = 1)\n",
    "\n",
    "        #make dataframe with individual criterion alongside prediction\n",
    "        pred_df = pd.DataFrame({'Criterion': list(criteria), 'Prediction': mostlikelyclass})\n",
    "\n",
    "        #gather associated probabilities from each prediction \"winner\" and add to list\n",
    "        winners = []\n",
    "        for x in range(len(pred_df)):\n",
    "            label = pred_df['Prediction'][x]\n",
    "            winner = prob_df[label][x]\n",
    "            winners.append(winner)\n",
    "\n",
    "        #for each prediction, if confidence less than 20%, assign as other\n",
    "        threshold = 0.2\n",
    "        for x in range(len(pred_df)):\n",
    "            if winners[x] < threshold:\n",
    "                pred_df[\"Prediction\"][x] = \"Other\"\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #for CNS involvement, drop predictions if CNS not in criterion (high sensitivity, low specificity)   \n",
    "        for x in range(len(pred_df)):\n",
    "            if (pred_df[\"Prediction\"][x] == \"CNSInvolvement\") and (\"CNS\" not in pred_df['Criterion'][x]):\n",
    "                pred_df[\"Prediction\"][x] = \"Other\"\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #return dataframe of original criteria with associated predictions\n",
    "        return pred_df\n",
    "    \n",
    "    #define ComputeMatchScore()\n",
    "    def ComputeMatchScore(self, patient, ExtractedCriteria, trialinfo, classified_df):\n",
    "\n",
    "        #counters to keep track of matches vs. potential matches\n",
    "        potentials = 0\n",
    "        matches = 0\n",
    "\n",
    "        #Age handler\n",
    "        #if age already specified in info dictionary, add as potential match\n",
    "        if (trialinfo['./eligibility/minimum_age'] != None) and (trialinfo['./eligibility/maximum_age'] != None):\n",
    "            potentials += 1\n",
    "\n",
    "        #otherwise, go looking to find age elsewhere\n",
    "        else:\n",
    "\n",
    "            #if age found in list of extracted criteria\n",
    "            for each in ExtractedCriteria:\n",
    "                if (type(each) == str) and (each.startswith(\"\\nAge\")):\n",
    "\n",
    "                    #regex to match comparator, number, and units\n",
    "                    regex = r'([<>≤≥]|greater than|less than|greater than or equal to|less than or equal to|over|under) {0,}[=]{0,} {0,}[0-9]{1,} {1,}(years|months|days)'\n",
    "\n",
    "\n",
    "                    #if regex match found, add as a potential match\n",
    "                    if re.search(regex, each) != None:\n",
    "                        potentials += 1\n",
    "                        for found in re.finditer(regex, each):\n",
    "\n",
    "                            #grab matching slice from criterion string\n",
    "                            age_slice = each[found.span()[0]:found.span()[1]].replace(' ', '')\n",
    "\n",
    "                            #if greater than symbol, add as minimum age to trial info dict\n",
    "                            if (\">\" in age_slice) or (\"greater\" in age_slice) or (\"over\" in age_slice):\n",
    "                                trialinfo['./eligibility/minimum_age'] = age_slice\n",
    "\n",
    "                            #otherwise, add as maximum age to trial info dict\n",
    "                            else:\n",
    "                                trialinfo['./eligibility/maximum_age'] = age_slice\n",
    "                                \n",
    "                    #look for \"and under\" syntax\n",
    "                    else:\n",
    "                        \n",
    "                        #regex search to match number, units, comparator\n",
    "                        regex2 = r'[0-9]{1,} {1,}(years|months|days){,1} {0,}and (under|over)'\n",
    "                        found = re.search(regex2, each)\n",
    "                        \n",
    "                        #if match, grab age slice and add as min/max age\n",
    "                        if found != None:\n",
    "                            potentials += 1\n",
    "                            age_slice = each[found.span()[0]:found.span()[1]].replace(' ', '')                         \n",
    "                            if \"under\" in age_slice:\n",
    "                                trialinfo['./eligibility/maximum_age'] = age_slice\n",
    "                            else:\n",
    "                                trialinfo['./eligibility/minimum_age'] = age_slice\n",
    "                            \n",
    "\n",
    "\n",
    "            #if age is still empty, go looking for age based on NLP-classifier \n",
    "            if (trialinfo['./eligibility/minimum_age'] == None) and (trialinfo['./eligibility/maximum_age'] == None):               \n",
    "\n",
    "                #if at least one criterion classified as \"Age\", add as a potential match\n",
    "                if len(classified_df[classified_df['Prediction'] == 'Age']) > 0:\n",
    "                    potentials += 1\n",
    "\n",
    "                    #for each potential match\n",
    "                    for each in classified_df[classified_df['Prediction'] == 'Age']['Criterion']:\n",
    "\n",
    "                        #regex to match comparator, number, and units\n",
    "                        regex = r'([<>≤≥]|greater than|less than|greater than or equal to|less than or equal to|over|under) {0,}[=]{0,} {0,}[0-9]{1,} {1,}(years|months|days)'\n",
    "\n",
    "                        #if regex match found\n",
    "                        if re.search(regex, each) != None:\n",
    "                            for found in re.finditer(regex, each):\n",
    "\n",
    "                                #grab matching slice from criterion string\n",
    "                                age_slice = each[found.span()[0]:found.span()[1]].replace(' ', '')\n",
    "\n",
    "                                #if greater than symbol, add as minimum age to trial info dict\n",
    "                                if (\">\" in age_slice) or (\"greater\" in age_slice) or (\"over\" in age_slice):\n",
    "                                    trialinfo['./eligibility/minimum_age'] = age_slice\n",
    "\n",
    "                                #otherwise, add as maximum age to trial info dict\n",
    "                                else:\n",
    "                                    trialinfo['./eligibility/maximum_age'] = age_slice\n",
    "\n",
    "                        #look for \"and under\" syntax\n",
    "                        else:\n",
    "\n",
    "                            #regex search to match number, units, comparator\n",
    "                            regex2 = r'[0-9]{1,} {1,}(years|months|days){,1} {0,}and (under|over)'\n",
    "                            found = re.search(regex2, each)\n",
    "\n",
    "                            #if match, grab age slice and add as min/max age\n",
    "                            if found != None:\n",
    "                                age_slice = each[found.span()[0]:found.span()[1]].replace(' ', '')\n",
    "                                if \"under\" in age_slice:\n",
    "                                    trialinfo['./eligibility/maximum_age'] = age_slice\n",
    "                                else:\n",
    "                                    trialinfo['./eligibility/minimum_age'] = age_slice\n",
    "\n",
    "        #minimum age\n",
    "        min_age_original = trialinfo[\"./eligibility/minimum_age\"]\n",
    "        \n",
    "        #if n/a or empty, min age is 0\n",
    "        if min_age_original == \"N/A\" or min_age_original == None:\n",
    "            min_age = 0.00\n",
    "        \n",
    "        #otherwise, detect units and convert to days\n",
    "        else:\n",
    "            if \"year\" in min_age_original.lower():\n",
    "                min_age = float(re.sub(r\"[^0-9]\", \"\", min_age_original))\n",
    "                min_age = min_age * 365\n",
    "            elif \"month\" in min_age_original.lower():\n",
    "                min_age = float(re.sub(r\"[^0-9]\", \"\", min_age_original))\n",
    "                min_age = min_age * 30\n",
    "            elif \"day\" not in min_age_original.lower():\n",
    "                min_age = float(re.sub(r\"[^0-9]\", \"\", min_age_original))\n",
    "                min_age = min_age * 365                 \n",
    "            else:\n",
    "                min_age = float(re.sub(r\"[^0-9]\", \"\", min_age_original))\n",
    "\n",
    "\n",
    "        #maximum age\n",
    "        max_age_original = trialinfo[\"./eligibility/maximum_age\"]\n",
    "        \n",
    "        #if n/a or empty, max age is 200 yrs\n",
    "        if max_age_original == \"N/A\" or max_age_original == None:\n",
    "            max_age = 200.00 * 365\n",
    "        \n",
    "        #otherwise, detect units and convert to days\n",
    "        else:\n",
    "            if \"year\" in max_age_original.lower():\n",
    "                max_age = float(re.sub(r\"[^0-9]\", \"\", max_age_original))\n",
    "                max_age = max_age * 365\n",
    "            elif \"month\" in max_age_original.lower():\n",
    "                max_age = float(re.sub(r\"[^0-9]\", \"\", max_age_original))\n",
    "                max_age = max_age * 30\n",
    "            elif \"day\" not in max_age_original.lower():\n",
    "                max_age = float(re.sub(r\"[^0-9]\", \"\", max_age_original))\n",
    "                max_age = max_age * 365                \n",
    "            else:\n",
    "                max_age = float(re.sub(r\"[^0-9]\", \"\", max_age_original))  \n",
    "\n",
    "        #compute if age match\n",
    "        if min_age <= patient[\"Age (Days)\"] <= max_age:\n",
    "            matches +=1\n",
    "\n",
    "        #Renal Function handler\n",
    "        for each in classified_df[classified_df['Prediction'] == 'RenalFunction']['Criterion']:\n",
    "            regex = r'(Glomerular filtration rate|glomerular filtration rate|GFR|\\(GFR\\)|Creatinine clearance|creatinine clearance|Creatinine clearance of|creatinine clearance of|creatinine-clearance|GRF|CrCl) {0,}(must be){,1} {0,}([<>≤≥]|>=|<=|greater than|greater than or equal to|at least){,1} {0,}[0-9]{2,3}'\n",
    "            if re.search(regex, each) != None:\n",
    "                potentials += 1\n",
    "\n",
    "                #GFR calculation\n",
    "\n",
    "                #for children less than 18\n",
    "                if patient[\"Age (Days)\"] < (18*365):\n",
    "\n",
    "                    #calculate GFR using Schwartz equation        \n",
    "                    GFR = (0.41 * patient[\"Height (cm)\"]) / patient[\"Creatinine (mg/dL)\"]\n",
    "\n",
    "                #for children greater than 18\n",
    "                else:\n",
    "\n",
    "                    #calculate GFR using MDRD equation        \n",
    "                    GFR = 175 * (patient[\"Creatinine (mg/dL)\"]**(-1.154)) * (patient[\"Age (Days)\"]**(-0.203))\n",
    "                    if patient[\"Female\"] == True:\n",
    "                        GFR = GFR * 0.742\n",
    "                    if patient[\"African-American\"] == True:\n",
    "                        GFR = GFR * 1.212\n",
    "\n",
    "                #find GFR requirement\n",
    "                found = re.search(regex, each)\n",
    "                gfr_slice = each[found.span()[0]:found.span()[1]].replace(' ', '')\n",
    "                gfr_slice = float(re.sub(r\"[^0-9]\", \"\", gfr_slice))\n",
    "\n",
    "                #if GFR greater than or equal to requirement, match is found\n",
    "                if GFR >= gfr_slice:\n",
    "                    matches += 1\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            #else look for serum creatinine\n",
    "            else:\n",
    "                regex2 = r'(Creatinine|creatinine) {0,}(must be){,1} {0,}([<>≤≥]|>=|<=|no greater than|less than|less than or equal to){,1} {0,}[0-9]\\.{,1}[0-9]{,1}'\n",
    "                found = re.search(regex2, each)\n",
    "                \n",
    "                #if found, add a potential match\n",
    "                if found != None:\n",
    "                    potentials += 1\n",
    "                    \n",
    "                    #if creatinine matches, add match\n",
    "                    cr_slice = each[found.span()[0]:found.span()[1]].replace(' ', '')\n",
    "                    cr_slice = float(re.sub(r\"[^0-9.]\", \"\", cr_slice))\n",
    "                    if patient[\"Creatinine (mg/dL)\"] <= cr_slice:\n",
    "                        matches += 1\n",
    "                        \n",
    "                #lastly, look for \"normal\" (assuming ULN serum Cr is 1)\n",
    "                else:\n",
    "                    \n",
    "                    #if normal in phrase add potential match\n",
    "                    if (\"normal\" in each) or (\"ULN\" in each):\n",
    "                        potentials += 1\n",
    "                        \n",
    "                        #if creatinine <= 1, add match\n",
    "                        if patient[\"Creatinine (mg/dL)\"] <= 1.0:\n",
    "                            matches += 1\n",
    "\n",
    "\n",
    "        #performance status handler\n",
    "        for each in classified_df[classified_df[\"Prediction\"] == \"PerformanceStatus\"]['Criterion']:\n",
    "            \n",
    "            #look for first possible syntax\n",
    "            if re.search(r'(Karnofsky|karnofsky|Karnofsky \\(adult\\)|Lansky|lanksy|Lanksy Play|Lanksy \\(pediatric\\)) {0,}(index|Index|score|scores|performance status|performance status \\(PS\\)|Performance status|Performance status \\(PS\\)|performance score|performance score \\(PS\\)|Performance score|Performance Score|Performance Level|PS|Performance Status \\(KPS\\)|performance scale|activity assessment){,1} {0,}(of){,1} {0,}([<>≤≥]|greater than|greater than or equal to|above){,1}[=]{,1} {0,}[0-9]{2,3}', each) != None:\n",
    "                regex = r'(Karnofsky|karnofsky|Karnofsky \\(adult\\)|Lansky|lanksy|Lanksy Play|Lanksy \\(pediatric\\)) {0,}(index|Index|score|scores|performance status|performance status \\(PS\\)|Performance status|Performance status \\(PS\\)|performance score|performance score \\(PS\\)|Performance score|Performance Score|Performance Level|PS|Performance Status \\(KPS\\)|performance scale|activity assessment){,1} {0,}(of){,1} {0,}([<>≤≥]|greater than|greater than or equal to|above){,1}[=]{,1} {0,}[0-9]{2,3}'\n",
    "                potentials += 1\n",
    "\n",
    "                #find performance status requirement\n",
    "                for found in re.finditer(regex, each):\n",
    "                    ps_slice = each[found.span()[0]:found.span()[1]].replace(' ', '')\n",
    "                    ps_slice = float(re.sub(r\"[^0-9]\", \"\", ps_slice))\n",
    "                    if patient[\"Performance Status (Lanksy/Karnofsky)\"] >= ps_slice:\n",
    "                        matches += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            #check other syntax\n",
    "            elif re.search(r'[<>≤≥]{0,1}[=]{0,1} {0,}[0-9]{2}%{0,1} {0,1}(Karnofsky|karnofsky|Lansky|lanksy)', each) != None:\n",
    "                regex = r'[<>≤≥]{0,1}[=]{0,1} {0,}[0-9]{2}%{0,1} {0,1}(Karnofsky|karnofsky|Lansky|lanksy)'\n",
    "                potentials += 1\n",
    "\n",
    "                #find performance status requirement\n",
    "                for found in re.finditer(regex, each):\n",
    "                    ps_slice = each[found.span()[0]:found.span()[1]].replace(' ', '')\n",
    "                    ps_slice = float(re.sub(r\"[^0-9]\", \"\", ps_slice))\n",
    "                    if patient[\"Performance Status (Lanksy/Karnofsky)\"] >= ps_slice:\n",
    "                        matches += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            #otherwise if ECOG score instead of karnofsky/lansky\n",
    "            else:\n",
    "                regex = r'(ECOG|Zubrod|zubrod|\\(ECOG\\)) {0,}(score|scores|performance status|PS|PS of){,1} {0,}([0-9]-[0-9]|of 0, 1, or 2|categories 0, 1, or 2|<= 2|of 0 or 1|of 0, 1 or 2)'\n",
    "                \n",
    "                #if ECOG found, add to potentials\n",
    "                if re.search(regex, each) != None:\n",
    "                    potentials += 1\n",
    "\n",
    "                    #ECOG score\n",
    "                    if patient[\"Performance Status (Lanksy/Karnofsky)\"] in (10,20):\n",
    "                        ECOG = 4\n",
    "                    elif patient[\"Performance Status (Lanksy/Karnofsky)\"] in (30,40):\n",
    "                        ECOG = 3\n",
    "                    elif patient[\"Performance Status (Lanksy/Karnofsky)\"] in (50,60):\n",
    "                        ECOG = 2\n",
    "                    elif patient[\"Performance Status (Lanksy/Karnofsky)\"] in (70,80):\n",
    "                        ECOG = 1\n",
    "                    elif patient[\"Performance Status (Lanksy/Karnofsky)\"] in (90,100):\n",
    "                        ECOG = 0\n",
    "\n",
    "                    #find performance status requirement\n",
    "                    found = re.search(regex, each)\n",
    "                    ps_slice = each[found.span()[0]:found.span()[1]]\n",
    "                    ps_slice = re.sub(r\"[^0-9]\", \"\", ps_slice)\n",
    "                    max_ecog = max([int(number) for number in ps_slice])\n",
    "                    \n",
    "                    #if patients ECOG matches, add match\n",
    "                    if ECOG <= max_ecog:\n",
    "                        matches += 1\n",
    "                        \n",
    "        #diagnosis handler\n",
    "        \n",
    "        #if conditions are known (from ct.gov), compare patient condition to known\n",
    "        if trialinfo['condition'] != None:\n",
    "            potentials += 1\n",
    "            if patient['Diagnosis'] in trialinfo['condition']:\n",
    "                matches += 1\n",
    "        \n",
    "        #otherwise, find conditions in study from diagnosis or disease status sections of protocol\n",
    "        else:\n",
    "            \n",
    "            #empty list in which to store study conditions (condition = diagnosis)\n",
    "            trialinfo['condition'] = []\n",
    "            \n",
    "            #look in Diagnosis section\n",
    "            for each in ExtractedCriteria:\n",
    "                if (type(each)) == str and each.startswith(\"\\nDiagnosis\"):\n",
    "                    \n",
    "                    #split on line breaks\n",
    "                    for diagnosis in each.split(\"\\n\"):\n",
    "                        \n",
    "                        #search for diagnosis keywords and add to above list\n",
    "                        search = re.search(r'(relapsed|Relapsed|refractory|Refractory|resistant|Resistant) {0,}([A-Z]{2,5}|disease|leukemia)', diagnosis)\n",
    "                        if search != None:\n",
    "                            trialinfo['condition'].append(diagnosis[search.span()[0]:search.span()[1]])\n",
    "                        else:\n",
    "                            search = re.search(r'([A-Z]{2,5}|disease|leukemia) {0,}(relapsed|refractory|resistant)', diagnosis)\n",
    "                            if search != None:\n",
    "                                trialinfo['condition'].append(diagnosis[search.span()[0]:search.span()[1]])\n",
    "                            else:\n",
    "                                pass\n",
    "                \n",
    "                #look in Disease Status Section\n",
    "                elif (type(each)) == str and each.startswith(\"\\nDisease Status\"):\n",
    "                    \n",
    "                    #split on line breaks\n",
    "                    for diagnosis in each.split(\"\\n\"):\n",
    "                        \n",
    "                        #search for diagnosis keywords and add to above list\n",
    "                        search = re.search(r'(relapsed|Relapsed|refractory|Refractory|resistant|Resistant) {0,}([A-Z]{2,5}|disease|leukemia)', diagnosis)\n",
    "                        if search != None:\n",
    "                            trialinfo['condition'].append(diagnosis[search.span()[0]:search.span()[1]])\n",
    "                        else:\n",
    "                            search = re.search(r'([A-Z]{2,5}|disease|leukemia) {0,}(relapsed|refractory|resistant)', diagnosis)\n",
    "                            if search != None:\n",
    "                                trialinfo['condition'].append(diagnosis[search.span()[0]:search.span()[1]])\n",
    "                            else:\n",
    "                                pass\n",
    "                \n",
    "                #otherwise, pass\n",
    "                else:\n",
    "                    pass   \n",
    "            \n",
    "            #now that diagnoses have been searched for \n",
    "            if trialinfo['condition'] != None:\n",
    "                potentials += 1\n",
    "                \n",
    "                #if patient diagnosis in list, add match\n",
    "                if patient['Diagnosis'] in trialinfo['condition']:\n",
    "                    matches += 1\n",
    "\n",
    "        #CNS Involvement Handler\n",
    "        #for each classified as CNSInvolvement\n",
    "        for each in classified_df[classified_df[\"Prediction\"] == \"CNSInvolvement\"]['Criterion']:\n",
    "            \n",
    "            #look for CNS numeric status and compare if detected\n",
    "            search = re.search(r'CNS(-| ){,1}(1|2|3)', each)\n",
    "            if search != None:\n",
    "                potentials += 1\n",
    "                if patient[\"CNS Involvement (1/2/3)\"] <= int(re.sub(r'[^0-9]', '', each[search.span()[0]:search.span()[1]])):\n",
    "                    matches += 1\n",
    "            \n",
    "            #otherwise look for ineligibility in regards to isolated CNS disease\n",
    "            else:\n",
    "                search = re.search(r'(isolated|Isolated) {0,}(CNS|central)', each)\n",
    "                if search != None:\n",
    "                    potentials += 1\n",
    "                    if patient['Isolated CNS Disease'] == False:\n",
    "                        matches += 1\n",
    "                \n",
    "                #otherwise look for ineligibility regarding general CNS involvement\n",
    "                else:\n",
    "                    search = re.search(r'(no|No|No known){,1} {0,}(signs of|clinical signs of){,1} {0,}(uncontrolled|active|current){,1} {0,}(CNS|central nervous system|central nervous system \\(CNS\\)) {0,}(disease|involvement) {0,}(eligible|ineligible|not eligibile|excluded|allowed|not allowed|are not eligible){,1}', each)\n",
    "                    if search != None:\n",
    "                        potentials += 1\n",
    "                        cns_slice = each[search.span()[0]:search.span()[1]]\n",
    "                        \n",
    "                        #if CNS involvement excluded\n",
    "                        if re.search(r'(not eligibile|ineligible|excluded|not allowed|are not eligible)', cns_slice) != None:\n",
    "                            if patient[\"CNS Involvement (1/2/3)\"] == 1:\n",
    "                                matches += 1\n",
    "                        \n",
    "                        #if CNS involvement allowed\n",
    "                        elif re.search(r'(eligibile|allowed)', cns_slice) != None:\n",
    "                            matches += 1\n",
    "                        \n",
    "                        #otherwise, assume CNS criterion is exclusionary for involvement\n",
    "                        else:\n",
    "                            if patient[\"CNS Involvement (1/2/3)\"] == 1:\n",
    "                                matches += 1\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "\n",
    "        #prior therapy handler\n",
    "        #for each criterion classified as prior therapy\n",
    "        for each in classified_df[classified_df[\"Prediction\"] == \"PriorAntileukemicTherapy\"]['Criterion']:\n",
    "            \n",
    "            #define two flexible regular expressions to detect comparators/time\n",
    "            regex = r'(within|until|at least|greater than|more than|up to|within the past|has been|[>≥]|>=) {0,}([0-9]{1,3}|one|two|three|four|five|six) {0,}(day|week|month|hour)'            \n",
    "            regex2 = r'([0-9]{1,3}|one|two|three|four|five|six) {0,}(day|week|month|hour|days|weeks|months|hours) {0,}(before|prior|must have elapsed|have elapsed)'\n",
    "            \n",
    "            #define lists of keywords to identify each subcategory of antileukemic therapy\n",
    "            chemotherapy = ['chemotherapy', 'cytotoxic', 'myelosuppressive', 'retinoid', 'bine ']\n",
    "            biologic = ['antibody', 'biologic', 'immunotherapy', 'immunomodulat', 'mab ', 'anticd', 'cellular therapy', 'interferon']\n",
    "            growthfactor = ['growth factor', 'gmcsf', 'gcsf', 'filgrastim', 'sargramostim', 'epoetin alfa']\n",
    "            radiotherapy = ['radio', 'radia', 'xrt', 'cgy']\n",
    "            steroids = ['corticosteroid', 'isone', 'asone']\n",
    "            \n",
    "            #search text using first regular expression for time/comparator patterns\n",
    "            search = re.search(regex, each.lower())\n",
    "            \n",
    "            #if match detected\n",
    "            if search != None:\n",
    "                \n",
    "                #scan text and extract number - convert to days if necessary\n",
    "                extracted_text = each[search.span()[0]:search.span()[1]]\n",
    "                if \"day\" in extracted_text:\n",
    "                    extracted_num = int(re.sub(r'[^0-9]', '', extracted_text))\n",
    "                elif \"week\" in extracted_text:\n",
    "                    extracted_num = int(re.sub(r'[^0-9]', '', extracted_text)) * 7\n",
    "                elif \"month\" in extracted_text:\n",
    "                    extracted_num = int(re.sub(r'[^0-9]', '', extracted_text)) * 30\n",
    "                elif \"hour\" in extracted_text:\n",
    "                    extracted_num = int(re.sub(r'[^0-9]', '', extracted_text)) / 24\n",
    "                \n",
    "                #if subcategory is chemotherapy, compare to patient characteristics and compute match\n",
    "                if any(term in each.lower() for term in chemotherapy):\n",
    "                    potentials += 1\n",
    "                    if (patient[\"Days Since Cytotoxic Chemotherapy\"] >= extracted_num) or (patient[\"Days Since Cytotoxic Chemotherapy\"] == False):\n",
    "                        matches += 1\n",
    "\n",
    "                #if subcategory is biologics, compare to patient characteristics and compute match\n",
    "                elif any(term in each.lower() for term in biologic):\n",
    "                    potentials += 1\n",
    "                    if (patient[\"Days Since Biologic Therapy\"] >= extracted_num) or (patient[\"Days Since Biologic Therapy\"] == False):\n",
    "                        matches += 1\n",
    "\n",
    "                #if subcategory is growth factors, cimpare to patient characteristics and compute match\n",
    "                elif any(term in each.lower() for term in growthfactor):\n",
    "                    potentials += 1\n",
    "                    if (patient[\"Days Since Growth Factor Therapy\"] >= extracted_num) or (patient[\"Days Since Growth Factor Therapy\"] == False):\n",
    "                        matches += 1\n",
    "\n",
    "                #if subcategory is radiotherapy, cimpare to patient characteristics and compute match\n",
    "                elif any(term in each.lower() for term in radiotherapy):\n",
    "                    potentials += 1\n",
    "                    if (patient[\"Days Since Prior Radiotherapy\"] >= extracted_num) or (patient[\"Days Since Prior Radiotherapy\"] == False):\n",
    "                        matches += 1\n",
    "\n",
    "                #if subcategory is steroids, cimpare to patient characteristics and compute match\n",
    "                elif any(term in each.lower() for term in steroids):\n",
    "                    potentials += 1\n",
    "                    if (patient[\"Days Since Corticosteroids\"] >= extracted_num) or (patient[\"Days Since Corticosteroids\"] == False):\n",
    "                        matches += 1\n",
    "            \n",
    "            #otherwise check second regex\n",
    "            else:\n",
    "                \n",
    "                #search text using second regular expression for time/comparator patterns\n",
    "                search = re.search(regex2, each.lower())\n",
    "                if search != None:\n",
    "                    \n",
    "                    #scan text and extract number - convert to days if necessary\n",
    "                    extracted_text = each[search.span()[0]:search.span()[1]]\n",
    "                    if \"day\" in extracted_text:\n",
    "                        extracted_num = int(re.sub(r'[^0-9]', '', extracted_text))\n",
    "                    elif \"week\" in extracted_text:\n",
    "                        extracted_num = int(re.sub(r'[^0-9]', '', extracted_text)) * 7\n",
    "                    elif \"month\" in extracted_text:\n",
    "                        extracted_num = int(re.sub(r'[^0-9]', '', extracted_text)) * 30\n",
    "                    elif \"hour\" in extracted_text:\n",
    "                        extracted_num = int(re.sub(r'[^0-9]', '', extracted_text)) / 24\n",
    "\n",
    "                    #if subcategory is chemotherapy, compare to patient characteristics and compute match\n",
    "                    if any(term in each.lower() for term in chemotherapy):\n",
    "                        potentials += 1\n",
    "                        if (patient[\"Days Since Cytotoxic Chemotherapy\"] >= extracted_num) or (patient[\"Days Since Cytotoxic Chemotherapy\"] == False):\n",
    "                            matches += 1\n",
    "\n",
    "                    #if subcategory is biologics, compare to patient characteristics and compute match\n",
    "                    elif any(term in each.lower() for term in biologic):\n",
    "                        potentials += 1\n",
    "                        if (patient[\"Days Since Biologic Therapy\"] >= extracted_num) or (patient[\"Days Since Biologic Therapy\"] == False):\n",
    "                            matches += 1\n",
    "\n",
    "                    #if subcategory is growth factors, cimpare to patient characteristics and compute match\n",
    "                    elif any(term in each.lower() for term in growthfactor):\n",
    "                        potentials += 1\n",
    "                        if (patient[\"Days Since Growth Factor Therapy\"] >= extracted_num) or (patient[\"Days Since Growth Factor Therapy\"] == False):\n",
    "                            matches += 1\n",
    "\n",
    "                    #if subcategory is radiotherapy, cimpare to patient characteristics and compute match\n",
    "                    elif any(term in each.lower() for term in radiotherapy):\n",
    "                        potentials += 1\n",
    "                        if (patient[\"Days Since Prior Radiotherapy\"] >= extracted_num) or (patient[\"Days Since Prior Radiotherapy\"] == False):\n",
    "                            matches += 1\n",
    "\n",
    "                    #if subcategory is steroids, cimpare to patient characteristics and compute match\n",
    "                    elif any(term in each.lower() for term in steroids):\n",
    "                        potentials += 1\n",
    "                        if (patient[\"Days Since Corticosteroids\"] >= extracted_num) or (patient[\"Days Since Corticosteroids\"] == False):\n",
    "                            matches += 1\n",
    "\n",
    "        #hepatic function handler\n",
    "        #for each classified as hepatic function\n",
    "        for each in classified_df[classified_df[\"Prediction\"] == \"HepaticFunction\"]['Criterion']:\n",
    "            \n",
    "            #search first for direct bilirubin in units of X times upper limit of normal\n",
    "            regex = r'(direct|conjugated){,1} {0,}bilirubin {0,}(must be){,1} {0,}([<≤≥>]|<=|>=|greater than|greater than or equal to|less than|less than or equal to|below|above){,1} {0,}[0-9].{,1}[0-9]{0,}(x|X|times){0,} {0,}(x|X|times){0,} {0,}(upper limit of normal|uln)'\n",
    "            search = re.search(regex, each.lower())\n",
    "            \n",
    "            #if match detected, add to potentials and compute if match\n",
    "            if search != None:\n",
    "                potentials += 1\n",
    "                if patient[\"Direct Bilirubin Times ULN\"] <= float(re.sub(r'[^0-9.]', '', each[search.span()[0]:search.span()[1]])):\n",
    "                    matches += 1    \n",
    "            \n",
    "            #else, search for AST/ALT in units of X times upper limit of normal\n",
    "            else:\n",
    "                regex2 = r'(alt|ast|sgot|sgpt|transaminase|transaminases)[^a-zA-Z]{,1} {0,}(must be){,1} {0,}([<≤≥>]|<=|>=|greater than|greater than or equal to|less than|less than or equal to|below|above){,1} {0,}[0-9].{,1}[0-9]{0,}(x|X|times){0,} {0,}(x|X|times){0,} {0,}(upper limit of normal|uln)'\n",
    "                search = re.search(regex2, each.lower())\n",
    "                \n",
    "                #if match detected, add to potentials and compute if match     \n",
    "                if search != None:\n",
    "                    potentials += 1\n",
    "                    if patient[\"AST/ALT Times ULN\"] <= float(re.sub(r'[^0-9.]', '', each[search.span()[0]:search.span()[1]])):\n",
    "                        matches += 1  \n",
    "                \n",
    "                #else, search for direct bilirubin or AST/ALT in original units (i.e. mg/dL)    \n",
    "                else:\n",
    "                    regex3 = r'(direct bilirubin|conjugated bilirubin|alt|ast|sgot|sgpt|transaminase|transaminases)[^a-zA-Z]{,1} {0,}([<≤≥>]|<=|>=|greater than|greater than or equal to|less than|less than or equal to|below|above){,1} {0,}[0-9]{1,3}.{,1}[0-9]{0,} {0,}(mg/dl|u/l|mgdl|ul)'\n",
    "                    search = re.search(regex3, each.lower())\n",
    "                    \n",
    "                    #if search is not empty\n",
    "                    if search != None:\n",
    "                        \n",
    "                        #if bilirubin detected, add potential\n",
    "                        if \"bilirubin\" in each[search.span()[0]:search.span()[1]]:\n",
    "                            potentials += 1\n",
    "                            \n",
    "                            #multiply patient input by 0.4 (cutoff for ULN) and compare to trial criteria\n",
    "                            if (patient[\"Direct Bilirubin Times ULN\"] * 0.4) <= float(re.sub(r'[^0-9.]', '', each[search.span()[0]:search.span()[1]])):\n",
    "                                matches += 1\n",
    "                        \n",
    "                        #otherwise is AST/ALT, add potential\n",
    "                        else:\n",
    "                            potentials += 1\n",
    "                            \n",
    "                            #multiply patient input by 40 (cutoff for ULN) and compare to trial criteria\n",
    "                            if (patient[\"AST/ALT Times ULN\"] * 40) <= float(re.sub(r'[^0-9.]', '', each[search.span()[0]:search.span()[1]])):\n",
    "                                matches += 1\n",
    "\n",
    "        #fertility pregnancy and contraception handler\n",
    "        #set initial presence of fertility/pregnancy/contraception exclusion as false\n",
    "        fert_preg_contra = False\n",
    "        fert_excl_keywords = [\"not eligible\", \"ineligible\", \"not participate\", \"exclude\", \"pregnancy test\", \"agree\", \"require\", \"unwilling\", \"effective\", \"risk\", \"hcg\", \"adequate\"]\n",
    "        \n",
    "        #for each criterion classified into this category\n",
    "        for each in classified_df[classified_df[\"Prediction\"] == \"FertilityPregnancyContraception\"]['Criterion']:\n",
    "            \n",
    "            #if exclusionary keywords present, add potential and change above presence variable to True\n",
    "            if any(term in each.lower() for term in fert_excl_keywords):\n",
    "                if fert_preg_contra == False:\n",
    "                    potentials += 1\n",
    "                    fert_preg_contra = True\n",
    "        \n",
    "        #if there is an exclusionary criterion and the patient characteristic is false, add a match\n",
    "        if (fert_preg_contra == True) and (patient[\"Pregnant, Nursing, or Fertile and Unwilling to Use Contraception\"] == False):\n",
    "            matches += 1\n",
    "\n",
    "\n",
    "        #active infection handler\n",
    "        #set initial presence of active infection to false\n",
    "        active_infection = False\n",
    "        \n",
    "        #for each classified into this category\n",
    "        for each in classified_df[classified_df[\"Prediction\"] == \"ActiveInfection\"]['Criterion']:\n",
    "            \n",
    "            #if exclusionary keywords present, add potential\n",
    "            if (\"not eligible\" in each.lower()) or (\"ineligible\" in each.lower()) or (\"not participate\" in each.lower()) or (\"exclude\" in each.lower()):\n",
    "                if active_infection == False:\n",
    "                    potentials += 1\n",
    "                    active_infection = True\n",
    "            \n",
    "            #if blood culture pattern present, add potential\n",
    "            elif re.search(r'positive {0,}(.*){,1} {0,}blood culture', each.lower()) != None:\n",
    "                if active_infection == False:\n",
    "                    potentials += 1\n",
    "                    active_infection = True \n",
    "            \n",
    "            #otherwise if \"infection\" keyword is present without inclusionary words, assume this is exclusionary\n",
    "            elif (\"infection\" in each.lower()) and (\"include\" not in each.lower()) and (\"eligible\" not in each.lower()):\n",
    "                if active_infection == False:\n",
    "                    potentials += 1\n",
    "                    active_infection = True                    \n",
    "        \n",
    "        #if there is an infection exclusion and patient characteristic is false, add match\n",
    "        if (active_infection == True) and (patient[\"Active and/or Uncontrolled Viral, Bacterial, or Fungal Infection\"] == False):\n",
    "            matches += 1\n",
    "\n",
    "        #cardiac function handler\n",
    "        #set initial cardiac function exclusion requirement to false\n",
    "        cv_dysfunction = False\n",
    "        \n",
    "        #for each criterion classified into this category\n",
    "        for each in classified_df[classified_df[\"Prediction\"] == \"CardiovascularFunction\"]['Criterion']:\n",
    "            \n",
    "            #check for anthracycline exposure and add as potential match\n",
    "            if (\"lifetime exposure\" in each.lower()) and (\"rubicin\" in each.lower()):\n",
    "                if cv_dysfunction == False:\n",
    "                    potentials += 1\n",
    "                    cv_dysfunction = True\n",
    "                    \n",
    "            #check for exclusionary keywords and add as potential match\n",
    "            elif (\"not eligible\" in each.lower()) or (\"ineligible\" in each.lower()) or (\"not participate\" in each.lower()) or (\"exclude\" in each.lower()):\n",
    "                if cv_dysfunction == False:\n",
    "                    potentials += 1\n",
    "                    cv_dysfunction = True\n",
    "                    \n",
    "            #check for adequate cardiac function pattern and add as potential match\n",
    "            elif re.search(r'adequate {0,}(.*){,1} {0,}function', each.lower()) != None:\n",
    "                if cv_dysfunction == False:\n",
    "                    potentials += 1\n",
    "                    cv_dysfunction = True \n",
    "            \n",
    "            #check for ejection fraction pattern and add as potential match\n",
    "            elif re.search(r'(reduc|shorten|low|poor)[a-z]{,5} {0,}(.*){,1} {0,}fraction', each.lower()) != None:\n",
    "                if cv_dysfunction == False:\n",
    "                    potentials += 1\n",
    "                    cv_dysfunction = True     \n",
    "            \n",
    "            #check for final keywords and add as potential match\n",
    "            elif (\"ejection fraction\" in each.lower()) or (\"stolic dysfunction\" in each.lower()):\n",
    "                if cv_dysfunction == False:\n",
    "                    potentials += 1\n",
    "                    cv_dysfunction = True                    \n",
    "        \n",
    "        #if there is a cardiac exclusion and patient characteristic is false, add match\n",
    "        if (cv_dysfunction == True) and (patient[\"Impaired Cardiovascular Function/Cardiotoxicity from Chemotherapy\"] == False):\n",
    "            matches += 1\n",
    "\n",
    "        #return match score - % of potential matches * % of criteria not classified as \"Other\"\n",
    "        match_score = (matches/potentials)*(len(classified_df[classified_df['Prediction'] != \"Other\"]) / len(classified_df))\n",
    "        return round(match_score, 2)\n",
    "    \n",
    "    #define Match()\n",
    "    def Match(self, patient, docx_trials, ctgov_trials):\n",
    "        \n",
    "        #empty lists in which to store trial names and scores\n",
    "        trials = []\n",
    "        scores = []\n",
    "\n",
    "        #if at least one .docx trial\n",
    "        if len(docx_trials) > 0:\n",
    "\n",
    "            #for each .docx trial\n",
    "            for trial in docx_trials:\n",
    "\n",
    "                #extract trial info\n",
    "                trialinfo = self.ExtractTrialInfo(path = trial)\n",
    "                \n",
    "                #extract raw criteria\n",
    "                ExtractedCriteria = self.ExtractCriteria(text = trialinfo['./eligibility/criteria/textblock'], mode = 'docx')\n",
    "                \n",
    "                #extract clean inclusion and exclusion criteria, dropping any rows with empty values\n",
    "                cleaninclusion = self.CleanCriteria(ExtractedCriteria[-2])\n",
    "                cleaninclusion = cleaninclusion[cleaninclusion['Final'] != '']\n",
    "                cleanexclusion = self.CleanCriteria(ExtractedCriteria[-1])\n",
    "                cleanexclusion = cleanexclusion[cleanexclusion['Final'] != '']\n",
    "                \n",
    "                #fit clean criteria into FastText embeddings\n",
    "                embedded_inclusion = self.EmbedCriteria(CleanedCriteria = cleaninclusion['Final'])\n",
    "                embedded_exclusion = self.EmbedCriteria(CleanedCriteria = cleanexclusion['Final'])\n",
    "                \n",
    "                #classify criteria and combine into single df\n",
    "                classified_df_in = self.ClassifyCriteria(criteria = cleaninclusion['Original'], embeddings = embedded_inclusion['Embedding'], model_folder_path = match_instance.classifier_model)\n",
    "                classified_df_ex = self.ClassifyCriteria(criteria = cleanexclusion['Original'], embeddings = embedded_exclusion['Embedding'], model_folder_path = match_instance.classifier_model)\n",
    "                classified_df = pd.concat([classified_df_in, classified_df_ex])\n",
    "                \n",
    "                #append trial name and match score to lists\n",
    "                trials.append(trialinfo['NCT_id'])\n",
    "                scores.append(self.ComputeMatchScore(patient = patient, trialinfo = trialinfo, ExtractedCriteria = ExtractedCriteria, classified_df = classified_df))\n",
    "\n",
    "        #if at least one ctgov trial\n",
    "        if len(ctgov_trials) > 0:\n",
    "\n",
    "            #for each ctgov trial\n",
    "            for trial in ctgov_trials:\n",
    "\n",
    "                #extract trial info\n",
    "                trialinfo = self.ExtractTrialInfo(path = trial)\n",
    "                \n",
    "                #extract raw criteria\n",
    "                ExtractedCriteria = self.ExtractCriteria(text = trialinfo['./eligibility/criteria/textblock'], mode = 'ctgov')\n",
    "                \n",
    "                #if lists in returned list, treat as inclusion/exclusion criteria\n",
    "                if True in [type(each) == list for each in ExtractedCriteria]:\n",
    "                \n",
    "                    #extract clean inclusion and exclusion criteria, dropping any rows with empty values\n",
    "                    cleaninclusion = self.CleanCriteria(ExtractedCriteria[-2])\n",
    "                    cleaninclusion = cleaninclusion[cleaninclusion['Final'] != '']\n",
    "                    cleanexclusion = self.CleanCriteria(ExtractedCriteria[-1])\n",
    "                    cleanexclusion = cleanexclusion[cleanexclusion['Final'] != '']\n",
    "\n",
    "                    #fit clean criteria into FastText embeddings\n",
    "                    embedded_inclusion = self.EmbedCriteria(CleanedCriteria = cleaninclusion['Final'])\n",
    "                    embedded_exclusion = self.EmbedCriteria(CleanedCriteria = cleanexclusion['Final'])\n",
    "\n",
    "                    #classify criteria and combine into single df\n",
    "                    classified_df_in = self.ClassifyCriteria(criteria = cleaninclusion['Original'], embeddings = embedded_inclusion['Embedding'], model_folder_path = self.classifier_model)\n",
    "                    classified_df_ex = self.ClassifyCriteria(criteria = cleanexclusion['Original'], embeddings = embedded_exclusion['Embedding'], model_folder_path = self.classifier_model)\n",
    "                    classified_df = pd.concat([classified_df_in, classified_df_ex])\n",
    "                    \n",
    "                #otherwise, treat as single list of criteria\n",
    "                else:\n",
    "                    \n",
    "                    #extract clean criteria, dropping rows with empty criteria\n",
    "                    cleancriteria = self.CleanCriteria(ExtractedCriteria)\n",
    "                    cleancriteria = cleancriteria[cleancriteria['Final'] != '']\n",
    "                    \n",
    "                    #fit clean criteria into FastText embeddings\n",
    "                    embedded = self.EmbedCriteria(CleanedCriteria = cleancriteria['Final'])\n",
    "\n",
    "                    #classify criteria and combine into single df\n",
    "                    classified_df = self.ClassifyCriteria(criteria = cleancriteria['Original'], embeddings = embedded['Embedding'], model_folder_path = self.classifier_model)\n",
    "\n",
    "                    \n",
    "                #append trial name and match score to lists\n",
    "                trials.append(trialinfo['NCT_id'])\n",
    "                scores.append(self.ComputeMatchScore(patient = patient, trialinfo = trialinfo, ExtractedCriteria = ExtractedCriteria, classified_df = classified_df))\n",
    "\n",
    "        #assemble matching score dataframe with ranked list of clinical trials        \n",
    "        match_df = pd.DataFrame({\"Trial\":trials, \"Match Score\":scores})\n",
    "        match_df = match_df.sort_values(by = ['Match Score'], ascending = False)\n",
    "        match_df = match_df.reset_index(drop = True)\n",
    "        return match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.9 ms, sys: 4.77 s, total: 4.81 s\n",
      "Wall time: 8.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#instantiate match instance with pretrained embedding and classifier models\n",
    "match_instance = gearboxNLP(embedding_model = \"/Users/Sam/Dropbox/Capstone/jupyter_notebooks/ft_embedding_size256_window5.model\",\n",
    "                            classifier_model = \"/Users/Sam/Dropbox/Capstone/classifier_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict for input patient characteristics\n",
    "patient = {\n",
    "           #patient's age in days\n",
    "           \"Age (Days)\": 7000,\n",
    "           \n",
    "           #patient's height in cm\n",
    "           \"Height (cm)\": 122,\n",
    "           \n",
    "           #patient's gender (female == True)\n",
    "           \"Female\": True,\n",
    "           \n",
    "           #patient's self-identified race (included to calculate eGFR using MDRD equation)\n",
    "           \"African-American\": False,\n",
    "    \n",
    "           #Lanksy or Karnofsky performance status scale (depending on age)\n",
    "           \"Performance Status (Lanksy/Karnofsky)\": 50,\n",
    "           \n",
    "           #patient's diagnosis \n",
    "           \"Diagnosis\": 'Refractory AML',\n",
    "           \n",
    "           #CNS involvement scale\n",
    "           \"CNS Involvement (1/2/3)\": 1,\n",
    "           \n",
    "           #presence of isolated CNS disease\n",
    "           \"Isolated CNS Disease\": False,\n",
    "           \n",
    "           #days since prior therapies as integer - input False for treatments the patients has not had\n",
    "           \"Days Since Cytotoxic Chemotherapy\": False,\n",
    "           \"Days Since Biologic Therapy\": 50,\n",
    "           \"Days Since Growth Factor Therapy\": 60,\n",
    "           \"Days Since Corticosteroids\": 40,\n",
    "           \"Days Since Prior Radiotherapy\": 10,\n",
    "           \n",
    "           #creatinine in mg/dL\n",
    "           \"Creatinine (mg/dL)\": 1.0,\n",
    "           \n",
    "           #hepatic function times ULN - e.g. if AST and/or ALT is 2x ULN, input 2\n",
    "           \"AST/ALT Times ULN\": 2,\n",
    "           \"Direct Bilirubin Times ULN\": 2,\n",
    "           \n",
    "           #true if reduced EF, high anthracycline exposure, etc.\n",
    "           \"Impaired Cardiovascular Function/Cardiotoxicity from Chemotherapy\": True,\n",
    "           \n",
    "           #true if patient current has an active infection (including HIV)\n",
    "           \"Active and/or Uncontrolled Viral, Bacterial, or Fungal Infection\": True,\n",
    "           \n",
    "           #true if patient is pregnant, nursing, or fertile and unwilling to use contraception during the study\n",
    "           \"Pregnant, Nursing, or Fertile and Unwilling to Use Contraception\": True}\n",
    "\n",
    "#file paths for trial protocols\n",
    "docx_trials = ['/Users/Sam/Downloads/apal/APAL2020D_concept_venetoclax_clean_03-17-2021  eak.docx',\n",
    "               '/Users/Sam/Downloads/apal/APAL2020E_Full Concept_Trametinib_210127.docx',\n",
    "               '/Users/Sam/Downloads/apal/APAL2020F_Concept_Flotetuzumab_11.30.20.docx']\n",
    "\n",
    "#NCT ID's for clinicaltrials.gov trials\n",
    "ctgov_trials = ['NCT03817320', 'NCT04726241']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.17 s, sys: 296 ms, total: 4.47 s\n",
      "Wall time: 5.81 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial</th>\n",
       "      <th>Match Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APAL2020E</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APAL2020F</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APAL2020D</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT03817320</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT04726241</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Trial  Match Score\n",
       "0    APAL2020E         0.49\n",
       "1    APAL2020F         0.43\n",
       "2    APAL2020D         0.42\n",
       "3  NCT03817320         0.31\n",
       "4  NCT04726241         0.19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#use Match() method to return ranked list of trials for this patient\n",
    "match_instance.Match(patient = patient,\n",
    "                     docx_trials = docx_trials,\n",
    "                     ctgov_trials = ctgov_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
