{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Individual Eligibility Criteria from ClinicalTrials.gov in Python\n",
    "## Author: Sam Kaskovich\n",
    "\n",
    "### Description:\n",
    "- This notebook extracts individual-level eligibility criteria for all non-actively enrolling pediatric acute leukemia trials on ClinicalTrials.gov. Clinical trial protocols can be downloaded as XML files, and eligibility criteria can be extracted from these files as a single block of free text.\n",
    "- Splitting each block of free text into individual criteria is not trivial, given that there is no single standardized format in which the criteria are listed. Criteria may appear as a bulleted list, bulleted list with related subbullets, outline, etc.\n",
    "- The following steps were undertaken to tackle this problem:\n",
    "    - 216 XML files for all non-actively enrolling clinical trials for pediatric acute leukemia were downloaded from ClinicalTrials.gov, using the instructions listed at this link: https://clinicaltrials.gov/ct2/resources/download\n",
    "    - The eligibility criteria free text block for each trial was manually inspected in order to ascertain patterns in formatting. The following four patterns emerged as dominant and each trial was manually labeled as such:<br><br>\n",
    "        - *Major Header/Subheader/Subbullets*:\n",
    "            - DISEASE CHARACTERISTICS:\n",
    "                - Criterion\n",
    "                - Criterion\n",
    "                - Etc\n",
    "            - PATIENT CHARACTERISTICS:\n",
    "                - Age\n",
    "                    - Criterion\n",
    "                - Performance status\n",
    "                    - Criterion\n",
    "                        - SubCriterion\n",
    "                - Etc\n",
    "            - PRIOR CONCURRENT THERAPY:\n",
    "                - Biologic therapy\n",
    "                    - Criterion\n",
    "                - Etc\n",
    "            - Etc<br><br>\n",
    "        - *Major Header/Subbullets*\n",
    "            - DISEASE CHARACTERISTICS:\n",
    "                - Criterion\n",
    "                - Criterion\n",
    "                    - SubCriterion\n",
    "                - Etc\n",
    "            - PATIENT CHARACTERISTICS:\n",
    "                - Criterion\n",
    "                - Criterion\n",
    "                - Etc\n",
    "            - PRIOR CONCURRENT THERAPY:\n",
    "                - Criterion\n",
    "                - Criterion\n",
    "                - Etc\n",
    "            - Etc<br><br>\n",
    "        - *Inclusion and/or Exclusion Criteria/No Nested Subbullets*\n",
    "            - Inclusion Criteria:\n",
    "                - Criterion\n",
    "                - Criterion\n",
    "                - Etc\n",
    "            - Exclusion Criteria:\n",
    "                - Criterion\n",
    "                - Criterion\n",
    "                - Etc<br><br>\n",
    "        - *Inclusion and Exclusion Criteria/Nested Subbullets*\n",
    "            - Inclusion Criteria:\n",
    "                - Criterion\n",
    "                - Criterion\n",
    "                    - SubCriterion\n",
    "                    - SubCriterion\n",
    "                    - Etc\n",
    "                - Etc\n",
    "            - Exclusion Criteria:\n",
    "                - Criterion\n",
    "                - Criterion\n",
    "                    - SubCriterion\n",
    "                    - SubCriterion\n",
    "                    - Etc\n",
    "                - Etc<br><br>\n",
    "    - A function [ExtractCriteria()] was written to parse each text block according to its format and then extract as many intact criteria as possible.\n",
    "    - NOTE: The goal of this process was not to extract each individual criterion perfectly, but to extract as many as feasible given the highly non-standardized nature of the free text blocks. Thus, a small number of criteria may be extracted as a large chunk, given that they have idiosyncratic structure or lack detectable common split character.\n",
    "\n",
    "### Notebook Format\n",
    "- Running the notebook requires the folder of XML files ('trial_protocols').\n",
    "- The notebook then can simply be run cell-by-cell from start to finish in order to reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas, numpy, regex, Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define RepeatRegexFinder function\n",
    "def RepeatRegexFinder(text, regex):\n",
    "    \"\"\"This function uses the Python regular expressions library to parse\n",
    "    a text block and return a list of subcontents that span a repeated regular\n",
    "    expression (i.e. returns the substring between each repeat of the given\n",
    "    regular expression pattern). Arguments are the text block (string) and \n",
    "    the desired regular expression (raw string) that demarcates the subcontents.\"\"\"\n",
    "    \n",
    "    #create empty lists in which to store starts and stops\n",
    "    starts = []\n",
    "    stops = []\n",
    "\n",
    "    #iterate over text to find indices matching regex and its start and stops\n",
    "    for match in re.finditer(regex, text):\n",
    "\n",
    "        #note start and stop of matching sequence\n",
    "        starts.append(match.span()[0])\n",
    "        stops.append(match.span()[1])\n",
    "\n",
    "    #create empty list in which to store subcontents\n",
    "    subcontents = []\n",
    "\n",
    "    #loop through starts and stops and add desired slices to list--leaving last subcontent off\n",
    "    for x in range(len(starts) - 1):\n",
    "\n",
    "        #store each mainbullet slice\n",
    "        subcontent = text[stops[x]:starts[x + 1]]\n",
    "        subcontents.append(subcontent)\n",
    "\n",
    "    #add last subcontent as last slice from 'text'\n",
    "    lastsubcontent = text[stops[-1]:]\n",
    "    subcontents.append(lastsubcontent)\n",
    "    \n",
    "    #return list of subcontents\n",
    "    return subcontents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define MultiRegexFinder() function\n",
    "def MultiRegexFinder(text, regex_list):\n",
    "    \"\"\"This function uses the Python regular expressions library to parse\n",
    "    a text block and return a list of subcontents between a list of unique\n",
    "    regular expression patterns (i.e. returns the substring between each \n",
    "    unique given regular expression pattern, if it exists in the text). \n",
    "    Arguments are the text block (string) and the desired regular expressions\n",
    "    (list of raw strings) that demarcate the subcontents.\"\"\"\n",
    "  \n",
    "    #create empty list in which to store indices for starts of regex's\n",
    "    starts = []\n",
    "\n",
    "    #iterate through regex's\n",
    "    for regex in regex_list:\n",
    "\n",
    "        #compile each regex\n",
    "        re_compiled = re.compile(regex)\n",
    "\n",
    "        #search text for regex\n",
    "        re_search = re_compiled.search(text)\n",
    "\n",
    "        #if search is not empty, add start to above empty list\n",
    "        if re_search != None:\n",
    "            start = re_search.span()[0]\n",
    "            starts.append(start)\n",
    "\n",
    "        #otherwise, do nothing\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #create empty list in which to store subcontents attached to each regex\n",
    "    subcontents = []\n",
    "\n",
    "    #loop through starts and add desired content slices to list--leaving last bullet off\n",
    "    for x in range(len(starts) - 1):\n",
    "\n",
    "        #add individual content slice of text to above list\n",
    "        subcontent = text[starts[x]:starts[x + 1]]\n",
    "        subcontents.append(subcontent)\n",
    "\n",
    "    #add last header content slice to list\n",
    "    lastsubcontent = text[starts[-1]:]\n",
    "    subcontents.append(lastsubcontent)\n",
    "    \n",
    "    #return subcontents\n",
    "    return subcontents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define ExtractCriteria()\n",
    "def ExtractCriteria(trial, text, output_list, format_dict):\n",
    "    \"\"\"This function will extract clinical trial criteria given the NCT ID (str),\n",
    "    text format (str), text block (str), and output list (list). It REQUIRES the\n",
    "    prior importation of the Python regular expressions library (re) and the prior\n",
    "    definition of RepeatRegexFinder() and MultiRegexFinder().\"\"\"\n",
    "\n",
    "    #create lists of headers for later use in format detection\n",
    "    majorheaders = [r'DISEASE CHARACTERISTICS', r'PATIENT CHARACTERISTICS', r'PRIOR CONCURRENT THERAPY', r'DONOR CHARACTERISTICS']\n",
    "    patientchars = [r'Age', r'Performance status', r'Life expectancy', r'Hematopoietic', r'Hepatic', r'Renal', r'Cardiovascular', r'Pulmonary', r'Other']\n",
    "    priortherapy = [r'Biologic therapy', r'Chemotherapy', r'Endocrine therapy', r'Radiotherapy', r'Surgery', r'Other']\n",
    "    \n",
    "    #extraction for: Inclusion and Exclusion Criteria/No Nested Subbullets\n",
    "    #if inclusion/exclusion criteria followed by any multiple charcters then ':', AND no subbullets\n",
    "    if ((re.search(r\"inclusion criteria(.*):\", text.lower()) != None) or (re.search(r\"exclusion criteria(.*):\", text.lower()) != None) or ()) and (re.search(r'\\r\\n\\r\\n {15}\\S', text) == None):\n",
    "\n",
    "        #split text at at each bullet/number, almost always noted by double carriage return, and add to output list\n",
    "        text = text.split(\"\\r\\n\\r\\n\")\n",
    "\n",
    "        #add criteria to output list\n",
    "        for each in text:\n",
    "            output_list.append(each)      \n",
    "\n",
    "        #add to format dict\n",
    "        format_dict['Inclusion and Exclusion Criteria/No Nested Subbullets'] += 1\n",
    "    \n",
    "    #extraction for: Inclusion and Exclusion Criteria/Nested Subbullets\n",
    "    #if inclusion/exclusion criteria followed by any multiple charcters then ':', AND contains subbullets\n",
    "    elif ((re.search(r\"inclusion criteria(.*):\", text.lower()) != None) or (re.search(r\"exclusion criteria(.*):\", text.lower()) != None) or ()) and (re.search(r'\\r\\n\\r\\n {15}\\S', text) != None):\n",
    "\n",
    "        #use RepeatRegexFinder() to return list of mainbullets and their subcontents\n",
    "        #mainbullets are denoted by the pattern of double carriage return followed by exactly 10 spaces\n",
    "        mainbullets = RepeatRegexFinder(text = text, regex = r'\\r\\n\\r\\n {10}\\S')\n",
    "\n",
    "        #add criteria to output list\n",
    "        for each in mainbullets:\n",
    "            output_list.append(each)\n",
    "\n",
    "        #add to format dict\n",
    "        format_dict['Inclusion and Exclusion Criteria/Nested Subbullets'] += 1\n",
    "            \n",
    "    #extraction for: Major Header/Subbullets\n",
    "    #if contains major head and 0 or 1 capitalized subheaders\n",
    "    elif (any(term in text for term in majorheaders) and (np.count_nonzero([term in text for term in patientchars]) < 4)):\n",
    "        \n",
    "        #use MultiRegexFinder to return header subcontents\n",
    "        headercontents = MultiRegexFinder(text = text, regex_list = majorheaders)\n",
    "\n",
    "        #deal with subbullet handling in the same way used above\n",
    "        #iterate through each header content\n",
    "        for subtext in headercontents:\n",
    "\n",
    "            #if carriage returns exist\n",
    "            if re.search(r'\\r\\n\\r\\n {10}\\S', subtext) != None:\n",
    "\n",
    "                #use RepeatRegexFinder() to return list of mainbullets and their subcontents\n",
    "                #mainbullets are denoted by the pattern of double carriage return followed by exactly 10 spaces\n",
    "                subtexts = RepeatRegexFinder(text = subtext, regex = r'\\r\\n\\r\\n {10}\\S')\n",
    "\n",
    "                #add criteria to output list\n",
    "                for each in subtexts:\n",
    "                    output_list.append(each)\n",
    "\n",
    "            #otherwise has no bullets - split on capitalized words\n",
    "            else:\n",
    "                capital_words = re.findall( r\"\\b[A-Z][a-z]*\\b\", subtext)\n",
    "                subtexts = MultiRegexFinder(text = subtext, regex_list = capital_words)\n",
    "                for each in subtexts:\n",
    "                    output_list.append(each)\n",
    "\n",
    "        #add to format dict\n",
    "        format_dict['Major Header/Subbullets'] += 1\n",
    "            \n",
    "    #extraction for last remaining Format ID: Major Header/Subheader/Subbullets\n",
    "    #this process will be the most complex, as it has the most heterogeneity\n",
    "    #if contains major head and >=2 capitalized subheaders\n",
    "    elif (any(term in text for term in majorheaders)) and (np.count_nonzero([term in text for term in patientchars]) >= 4):\n",
    "\n",
    "        #use MultiRegexFinder to return major header subcontents\n",
    "        headercontents = MultiRegexFinder(text = text, regex_list = majorheaders)\n",
    "\n",
    "        #deal with subbullet/subheader handling for each major header section\n",
    "        #iterate through each header content        \n",
    "        for subtext in headercontents:\n",
    "\n",
    "            #if patient characteristics in subtext (regardless of capitalization pattern), handle as list with subheaders/bullets from patientchars\n",
    "            if \"patient characteristics\" in subtext.lower():\n",
    "\n",
    "                #use MultiRegexFinder() to return subcontents of patientchars headers\n",
    "                subcontents = MultiRegexFinder(text = subtext, regex_list = patientchars)\n",
    "\n",
    "                #add to raw criteria\n",
    "                for each in subcontents:\n",
    "                    output_list.append(each)\n",
    "\n",
    "            #if prior concurrent therapy in subtext (regardless of capitalization pattern), handle as bulleted list with subheaders listed above\n",
    "            #the \"chemotherapy\" specification was added to ensure this section occurs in subheader format\n",
    "            #when \"chemotherapy\" is not present, the format does not have subheaders and will be handled otherwise\n",
    "            elif (\"prior concurrent therapy\" in subtext.lower()) and (\"chemotherapy\" in subtext.lower()):\n",
    "\n",
    "                #use MultiRegexFinder() to return subcontents of priortherapy headers\n",
    "                subcontents = MultiRegexFinder(text = subtext, regex_list = priortherapy)\n",
    "\n",
    "                #add to output_list\n",
    "                for each in subcontents:\n",
    "                    output_list.append(each)\n",
    "\n",
    "            #otherwise, most can be treated as bulleted list with potential subbullets as with other format IDs\n",
    "            elif re.search(r'\\r\\n\\r\\n {10}\\S', subtext) != None:\n",
    "\n",
    "                #use RepeatRegexFinder() to return list of mainbullets and their subcontents\n",
    "                #mainbullets are denoted by the pattern of double carriage return followed by exactly 10 spaces\n",
    "                mainbullets = RepeatRegexFinder(text = subtext, regex = r'\\r\\n\\r\\n {10}\\S')\n",
    "\n",
    "                #add criteria to output list\n",
    "                for each in mainbullets:\n",
    "                    output_list.append(each)\n",
    "\n",
    "            #otherwise, subtext is a full paragraph without demarcation between individual criteria\n",
    "            else:\n",
    "\n",
    "                #add full subtext to output_list\n",
    "                output_list.append(subtext)\n",
    "\n",
    "        #add to format dict\n",
    "        format_dict['Major Header/Subheader/Subbullets'] += 1\n",
    "            \n",
    "    #otherwise, treat as bulleted list\n",
    "    else:\n",
    "\n",
    "        #split text at at each bullet/number, almost always noted by double carriage return, and add to output list\n",
    "        text = text.split(\"\\r\\n\\r\\n\")\n",
    "\n",
    "        #add criteria to output list\n",
    "        for each in text:\n",
    "            output_list.append(each)\n",
    "\n",
    "        #add to format dict\n",
    "        format_dict['Other'] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ElementTree from xml library to parse files   \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#create list of desired variables to extract from xml\n",
    "variables = [\"nct_id\", \"start_date\", \"phase\", \"./eligibility/gender\", \"./eligibility/minimum_age\", \"./eligibility/maximum_age\", \"./eligibility/criteria/textblock\"]\n",
    "\n",
    "#create dict in which each key is one of the variables\n",
    "variables_dict = {}.fromkeys(variables)\n",
    "\n",
    "#assign each key (i.e. variable) in dict a value of empty list\n",
    "for variable in variables_dict.keys():\n",
    "    variables_dict[variable] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over names of xml files\n",
    "for trial in os.listdir('/Users/Sam/Dropbox/Capstone/trials_protocols_extended'):\n",
    "\n",
    "    #create a parser called \"tree\" using .parse() and pass trial name\n",
    "    tree = ET.parse('/Users/Sam/Dropbox/Capstone/trials_protocols_extended/' + trial)\n",
    "    \n",
    "    #represent data as tree-like structure with .getroot()\n",
    "    root = tree.getroot()\n",
    "\n",
    "    #iterate through variables to extract\n",
    "    for variable in variables_dict.keys():\n",
    "        if variable == 'nct_id':\n",
    "            variables_dict[variable].append(trial.strip('.xml'))\n",
    "        else:\n",
    "            variables_dict[variable].append(root.findtext(variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count different formats detected\n",
    "format_dict = {label:0 for label in format_df.Label}\n",
    "format_dict['Other'] = 0\n",
    "\n",
    "#iterate through list containing free text blocks and replace with formatted text\n",
    "for x in range(len(os.listdir('/Users/Sam/Dropbox/Capstone/trials_protocols_extended'))):\n",
    "    \n",
    "    #extraction of raw criteria\n",
    "    output_list = []\n",
    "    trial = (os.listdir('/Users/Sam/Dropbox/Capstone/trials_protocols_extended')[x]).strip('.xml')\n",
    "    fulltext = variables_dict[\"./eligibility/criteria/textblock\"][x]\n",
    "    ExtractCriteria(trial = trial, text = fulltext, output_list = output_list, format_dict = format_dict)\n",
    "    variables_dict[\"./eligibility/criteria/textblock\"][x] = output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trials master df\n",
    "trials_df = pd.DataFrame(variables_dict)\n",
    "trials_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return format frequencies\n",
    "format_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv\n",
    "trials_df.to_csv(\"trial_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#store dates\n",
    "dates = []\n",
    "\n",
    "#original raw_criteria\n",
    "raw_criteria = []\n",
    "\n",
    "#empty list in which to store separated raw criteria\n",
    "raw_criteria_long = []\n",
    "\n",
    "#set regex as raw string for first subbullet\n",
    "regex = r'\\r\\n\\r\\n {15}\\S'\n",
    "\n",
    "#compile each regex\n",
    "re_compiled = re.compile(regex)\n",
    "\n",
    "#complex subbullet counter\n",
    "subbullet_counter = 0\n",
    "\n",
    "#iterate through trial df\n",
    "for x in range(len(trials_df)):\n",
    "    \n",
    "    #note date\n",
    "    date = trials_df['start_date'][x]\n",
    "    \n",
    "    #note full text block\n",
    "    fulltext = trials_df['./eligibility/criteria/textblock'][x]\n",
    "\n",
    "    #for each criterion in list of separated criteria\n",
    "    for text in fulltext:\n",
    "\n",
    "        #append to original raw criteria\n",
    "        raw_criteria.append(text)\n",
    "        \n",
    "        #search text for regex\n",
    "        re_search = re_compiled.search(text)\n",
    "\n",
    "        #if the first subbullet regex is found\n",
    "        if re_search != None:\n",
    "\n",
    "            #add to complex subbullet counter\n",
    "            subbullet_counter += 1\n",
    "\n",
    "            #create empty lists in which to store starts of subbullets\n",
    "            starts = []\n",
    "\n",
    "            #iterate over text to find indices matching regex and its start\n",
    "            for match in re.finditer(regex, text):\n",
    "\n",
    "                #note start and stop of matching sequence\n",
    "                starts.append(match.span()[0])\n",
    "\n",
    "            #define first \"base statement\" as substring with indices of first start and stop\n",
    "            base_statement = text[:starts[0]]\n",
    "\n",
    "            #for each subbullet detected\n",
    "            for x in range(len(starts) - 1):\n",
    "\n",
    "                #concatenate subbullet to base_statement and add to new list\n",
    "                new_criterion = base_statement + text[starts[x]:starts[x + 1]]\n",
    "                raw_criteria_long.append(new_criterion)\n",
    "                dates.append(date)\n",
    "        \n",
    "        #otherwise, ignore\n",
    "        else:\n",
    "            raw_criteria_long.append(text)\n",
    "            dates.append(date)\n",
    "            \n",
    "#break up 'Other' and \"Biologic therapy\" blocks\n",
    "raw_criteria_longer = []\n",
    "for each in raw_criteria_long:\n",
    "    \n",
    "    #strip leading/ending whitespace\n",
    "    stripped = each.strip()\n",
    "    \n",
    "    #biologic therapy\n",
    "    if stripped.startswith(\"Biologic therapy\", 0, len(\"Biologic therapy\")) == True:\n",
    "        criteria_split = each.split(\"\\r\\n\\r\\n          -\")\n",
    "        for criterion in criteria_split:\n",
    "            raw_criteria_longer.append(criterion)\n",
    "            \n",
    "    #other\n",
    "    elif stripped.startswith(\"Other\", 0, len(\"Other\")) == True:\n",
    "        criteria_split = each.split(\"\\r\\n\\r\\n          -\")\n",
    "        for criterion in criteria_split:\n",
    "            raw_criteria_longer.append(criterion)\n",
    "    \n",
    "    #otherwise, add criterion to list\n",
    "    else:\n",
    "        raw_criteria_longer.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw criteria:              {len(raw_criteria)}\")\n",
    "print(f\"Raw criteria long:         {len(raw_criteria_long)}\")\n",
    "print(f\"Raw criteria longer:       {len(raw_criteria_longer)}\")\n",
    "print(f\"Complex subbullets:        {subbullet_counter} ({(subbullet_counter/len(raw_criteria)*100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile regular expression to detect strings of all caps (i.e. abbreviations)\n",
    "capitalized_words = []\n",
    "regex = r\"\\b[A-Z]{2,}\\b\"\n",
    "\n",
    "#iterate through criteria\n",
    "for text in raw_criteria_longer:\n",
    "    \n",
    "    #iterate over each text to find indices matching regex and its start/stop\n",
    "    for match in re.finditer(regex, text):\n",
    "\n",
    "        #note start and stop of matching sequence\n",
    "        capitalized_word = text[(match.span()[0]):(match.span()[1])]\n",
    "        \n",
    "        #append to list\n",
    "        capitalized_words.append(capitalized_word)\n",
    "        \n",
    "#make frequency table\n",
    "top_caps = Counter(capitalized_words)\n",
    "top_caps.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty list in which to store cleaned text with the following changes\n",
    "raw_criteria_long_trimmed = []\n",
    "\n",
    "#compile regular expression to detect strings of all caps (i.e. abbreviations)\n",
    "regex = r\"\\b[A-Z]{2,}\\b\"\n",
    "re_compiled = re.compile(regex)\n",
    "\n",
    "#list of common all caps non-abbreviation words (appear > 1x)\n",
    "non_abrv = [\"DONOR\", \"DISEASE\", \"CHARACTERISTICS\", \"AND\", \"DONORS\", \"RELATED\", \"OR\", \"INCLUSION\", \"CRITERIA\", \"EXCLUSION\", \"PRIOR\", \"CONCURRENT\", \"THERAPY\", \"NOTE\", \"BEFORE\", \"PATIENTS\", \"MATCHED\", \"UNRELATED\", \"MUST\", \"REAL\", \"TRANSPLANT\", \"PATIENT\", \"ELIGIBILITY\", \"ALLOWED\", \"ADULT\", \"PEDIATRIC\", \"ORGAN\", \"DYSFUNCTION\", \"EXCEPT\", \"STRATUM\", \"STRATA\", \"GROUP\", \"AGED\"]\n",
    "\n",
    "#conduct pre-processing steps for each criterion in list\n",
    "for each in raw_criteria_longer:\n",
    "\n",
    "    #break criterion into single words\n",
    "    word_list = []\n",
    "    for word in each.split():\n",
    "        \n",
    "        #search word for abbreviations\n",
    "        re_search = re_compiled.search(word)\n",
    "        \n",
    "        #if search is not empty and word isn't a commonly all-caps non abbreviation\n",
    "        #keep abbreviation as is, otherwise lowercase\n",
    "        if (re_search != None) & (not any(term in word for term in non_abrv)):\n",
    "            word_list.append(word)\n",
    "        else:\n",
    "            word = word.lower()\n",
    "            word_list.append(word)\n",
    "        \n",
    "    #reassign \"each\" to sentence that is lowercased except for abbreviations\n",
    "    each = \" \".join(word_list)\n",
    "\n",
    "    #remove all special characters except numbers and ';' (often used in genetic mutations)\n",
    "    each = re.sub(r'[^A-z0-9 ;]', \"\", each)\n",
    "\n",
    "    #remove all single characters\n",
    "    each = re.sub(r'\\s+[a-zA-Z]\\s+', \"\", each)\n",
    "    \n",
    "    #replace multiple whitespace with single whitespace\n",
    "    each = re.sub(\" +\", \" \", each)\n",
    "    \n",
    "    #strip leading and ending whitespace\n",
    "    each = each.strip()\n",
    "\n",
    "    #add to new empty list\n",
    "    raw_criteria_long_trimmed.append(each)\n",
    "    \n",
    "#confirm same length\n",
    "print(f\"Raw criteria longer:       {len(raw_criteria_longer)}\")\n",
    "print(f\"Raw criteria long trimmed: {len(raw_criteria_long_trimmed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print frequency table\n",
    "criteria_freq = Counter(raw_criteria_long_trimmed)\n",
    "for each in criteria_freq.most_common():\n",
    "    print(f\"{each[0]}\\n{each[1]}\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on frequency table, assemble list of common phrases to be tossed out (single word phrases will later be removed)\n",
    "meaningless_criteria = [\"inclusion criteria\", \"exclusion criteria\", \"see disease characteristics\", \"not specified\", \"at least 8 weeks\"]\n",
    "\n",
    "#create empty list store indices of non-empty, non-single word criteria\n",
    "to_keep = []\n",
    "\n",
    "#iterate through criteria list \n",
    "for x in range(len(raw_criteria_long_trimmed)):\n",
    "    \n",
    "    #if empty string, pass\n",
    "    if not raw_criteria_long_trimmed[x]:\n",
    "        pass\n",
    "    \n",
    "    #if single word, pass\n",
    "    elif len(raw_criteria_long_trimmed[x].split()) == 1:\n",
    "        pass\n",
    "    \n",
    "    #if criterion in list of meaningless criteria, pass\n",
    "    elif raw_criteria_long_trimmed[x] in meaningless_criteria:\n",
    "        pass\n",
    "    \n",
    "    #else, add to raw_criteria_v2\n",
    "    else:\n",
    "        to_keep.append(x)\n",
    "\n",
    "print(f\"Raw criteria long trimmed: {len(raw_criteria_long_trimmed)}\")\n",
    "print(f\"Final criteria trimmed:    {len(to_keep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty lists to which original and trimmed (i.e. formatted for NLP)\n",
    "#criteria_original = []\n",
    "criteria_trimmed_stops = []\n",
    "\n",
    "#loop through indices of criteria to keep\n",
    "for index in to_keep:\n",
    "    \n",
    "    #add original and trimmed criteria, respectively\n",
    "    #criteria_original.append(raw_criteria_long[index])\n",
    "    criteria_trimmed_stops.append(raw_criteria_long_trimmed[index])\n",
    "    #final_dates.append(dates[index])\n",
    "    \n",
    "#check \n",
    "#print(final_dates[:50], criteria_original[:50], criteria_trimmed_stops[:50])\n",
    "#print(f\"Raw criteria long trimmed: {len(criteria_original)}\")\n",
    "print(f\"Final criteria trimmed:    {len(criteria_trimmed_stops)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of all words\n",
    "all_words = []\n",
    "for criterion in criteria_trimmed_stops:\n",
    "    for word in criterion.split():\n",
    "        all_words.append(word)\n",
    "        \n",
    "#create frequency table\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "#print 100 most common words\n",
    "for x in range(100):\n",
    "    print(word_freq.most_common()[x][0], end = \"','\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of custom stop words based on top 100 terms, many removed for semantic significance\n",
    "custom_stops = ['or','of','the','patients','to','for','with','no','and','at','not','must','be','have','in',\n",
    "                'are','than','as', 'by','is','study','other','on', 'who','if', 'will','any', 'criteria','patient',\n",
    "                'from','this','that','allowed','an','may','all','known']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list for nostops criteria\n",
    "criteria_trimmed_nostops = []\n",
    "\n",
    "#iterate through criteria, split into words, remove stops, and join back together\n",
    "for criterion in criteria_trimmed_stops:\n",
    "    listofwords = []\n",
    "    for word in criterion.split():\n",
    "        if word in custom_stops:\n",
    "            pass\n",
    "        else:\n",
    "            listofwords.append(word)\n",
    "    newcriterion = \" \".join(listofwords)\n",
    "    criteria_trimmed_nostops.append(newcriterion)  \n",
    "    \n",
    "#check length\n",
    "print(len(criteria_trimmed_nostops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list with stops removed, no empty strings or single words\n",
    "pre_lemmed = []\n",
    "\n",
    "#iterate through criteria list \n",
    "for criterion in criteria_trimmed_nostops:\n",
    "    \n",
    "    #if empty string, pass\n",
    "    if not criterion:\n",
    "        pass\n",
    "    \n",
    "    #if single word, pass\n",
    "    elif len(criterion.split()) == 1:\n",
    "        pass\n",
    "    \n",
    "    #else, add to raw_criteria_v2\n",
    "    else:\n",
    "        pre_lemmed.append(criterion)\n",
    "        \n",
    "print(len(pre_lemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Sam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/Sam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Sam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary objects/functions for lemmatization with parts of speech tagging\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "#define pos_tagger, which changes a given nltk tag to the wordnet abbrev\n",
    "def pos_tagger(nltk_tag): \n",
    "    if nltk_tag.startswith('J'): \n",
    "        return wordnet.ADJ \n",
    "    \n",
    "    elif nltk_tag.startswith('V'): \n",
    "        return wordnet.VERB \n",
    "    \n",
    "    elif nltk_tag.startswith('N'): \n",
    "        return wordnet.NOUN \n",
    "    \n",
    "    elif nltk_tag.startswith('R'): \n",
    "        return wordnet.ADV \n",
    "    \n",
    "    else:           \n",
    "        return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty list in which to store final, lemmatized criteria\n",
    "criteria_final = []\n",
    "\n",
    "#list of additional suffixes to be removed\n",
    "suffix_list = [\"tion\", \"ical\", \"ious\", \"ance\"]\n",
    "\n",
    "#iterate through criteria\n",
    "for sentence in pre_lemmed:\n",
    "    \n",
    "    # tokenize the sentence and find the POS tag for each token \n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))   \n",
    "\n",
    "    #use previously defined function to fix tags\n",
    "    #reference: https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/#:~:text=Wordnet%20Lemmatizer%20(with%20POS%20tag)&text=This%20is%20because%20these%20words,%2C%20noun%2C%20adjective%20etc).\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged)) \n",
    "\n",
    "    #create empty list in which to store lemmatized sentence\n",
    "    lemmatized_sentence = [] \n",
    "    \n",
    "    #iterate through mapped list with wordnet tags\n",
    "    for word, tag in wordnet_tagged: \n",
    "        #if there is no available tag, append the token as is \n",
    "        if tag is None: \n",
    "            lemmatized_sentence.append(word) \n",
    "        \n",
    "        # else use the tag to lemmatize the token         \n",
    "        else:         \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
    "    \n",
    "    #remove selected suffixes from words that are poorly handled by automatic lemmatizer\n",
    "    for index in range(len(lemmatized_sentence)):\n",
    "        if lemmatized_sentence[index][-4:] in suffix_list:\n",
    "            lemmatized_sentence[index] = lemmatized_sentence[index][:-4]\n",
    "    \n",
    "    #join previously created list into sentence (i.e. single string)\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence) \n",
    "\n",
    "    #add lemmatized sentence to finalized criteria list\n",
    "    criteria_final.append(lemmatized_sentence)\n",
    "    \n",
    "print(len(pre_lemmed))\n",
    "print(len(criteria_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_final_df = pd.DataFrame({'Criteria': criteria_final})\n",
    "criteria_final_df.to_csv(\"criteria_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Criteria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>diagnosis acute myeloid leukemia acute lymphob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>first subsequent relapse refractory disease af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>antecedent hematologic disorder except philade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>age 15 over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>perform status 03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Criteria\n",
       "0           0  diagnosis acute myeloid leukemia acute lymphob...\n",
       "1           1  first subsequent relapse refractory disease af...\n",
       "2           2  antecedent hematologic disorder except philade...\n",
       "3           3                                        age 15 over\n",
       "4           4                                  perform status 03"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria_final_df = pd.read_csv(\"criteria_final.csv\")\n",
    "criteria_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Criteria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>3050</td>\n",
       "      <td>1 21 year age when originally diagnose acute l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>2222</td>\n",
       "      <td>1 30 year old havebody weight 10 kg entry note...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>5206</td>\n",
       "      <td>1 31 year age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>932</td>\n",
       "      <td>1 APML diagnosis base upon morpholog histochem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>212</td>\n",
       "      <td>1 age 21 year age when enrol onto t2005001 pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           Criteria\n",
       "3050        3050  1 21 year age when originally diagnose acute l...\n",
       "2222        2222  1 30 year old havebody weight 10 kg entry note...\n",
       "5206        5206                                      1 31 year age\n",
       "932          932  1 APML diagnosis base upon morpholog histochem...\n",
       "212          212  1 age 21 year age when enrol onto t2005001 pro..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria_final_df = criteria_final_df.sort_values(by = [\"Criteria\"])\n",
    "criteria_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Criteria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>3050</td>\n",
       "      <td>21 year age when originally diagnose acute lym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>2222</td>\n",
       "      <td>30 year old havebody weight 10 kg entry note m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>5206</td>\n",
       "      <td>31 year age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>932</td>\n",
       "      <td>APML diagnosis base upon morpholog histochem a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>212</td>\n",
       "      <td>age 21 year age when enrol onto t2005001 proto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           Criteria\n",
       "3050        3050  21 year age when originally diagnose acute lym...\n",
       "2222        2222  30 year old havebody weight 10 kg entry note m...\n",
       "5206        5206                                        31 year age\n",
       "932          932  APML diagnosis base upon morpholog histochem a...\n",
       "212          212  age 21 year age when enrol onto t2005001 proto..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria_final_df['Criteria'] = criteria_final_df['Criteria'].str.replace('^[0-9]*', '', regex = True)\n",
    "criteria_final_df['Criteria'] = criteria_final_df['Criteria'].str.strip()\n",
    "criteria_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5251\n",
      "[['21', 'year', 'age', 'when', 'originally', 'diagnose', 'acute', 'lymphoblastic', 'leukemia', 'ALL'], ['30', 'year', 'old', 'havebody', 'weight', '10', 'kg', 'entry', 'note', 'more', '3', 'age', '21', '30', 'enrol'], ['31', 'year', 'age'], ['APML', 'diagnosis', 'base', 'upon', 'morpholog', 'histochem', 'andor', 'flow', 'cytometric', 'confirm', 'upon', 'review', 'bycentral', 'studydesignated', 'hematologic', 'pathologist', ';'], ['age', '21', 'year', 'age', 'when', 'enrol', 'onto', 't2005001', 'protocol', 'version', '6272007', '17']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize sentences for FastText input\n",
    "tokenized_final = [nltk.word_tokenize(criterion) for criterion in criteria_final_df.Criteria]\n",
    "\n",
    "print(len(tokenized_final))\n",
    "print(tokenized_final[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import FastText\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set FastText hyperparameters\n",
    "#defaults listed in this article: https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html\n",
    "#embedding size of 256 chosen as order of magnitude for 2^n\n",
    "data = tokenized_final\n",
    "embedding_size = 256\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-4\n",
    "alpha = 0.025\n",
    "model = 0\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 s, sys: 20.3 s, total: 48.4 s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#instantiate FastText model and print time\n",
    "ft_model = FastText(data,\n",
    "                    size = embedding_size,\n",
    "                    window = window_size,\n",
    "                    min_count = min_word,\n",
    "                    sample = down_sampling,\n",
    "                    sg = model,\n",
    "                    iter = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 450 ms, sys: 26.5 s, total: 26.9 s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ft_model.save(\"ft_embedding_size256_window5.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "testmodel = FastText.load(\"ft_embedding_size256_window5.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-738c77148bad>:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  sent_vec = model[w]\n",
      "<ipython-input-50-738c77148bad>:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  sent_vec = np.add(sent_vec, model[w])\n"
     ]
    }
   ],
   "source": [
    "#this function will turn individual word vectors into a sentence vector\n",
    "#accepts the sentence item (sent) and the FastText model (model)\n",
    "def sent_vectorizer(sent, model):\n",
    "    \n",
    "    #empty list in which to store sentence vectors\n",
    "    sent_vec =[]\n",
    "    \n",
    "    #keeps track of total number of words in sentence\n",
    "    numw = 0\n",
    "    \n",
    "    #for each word in a sentence\n",
    "    for w in sent:\n",
    "        \n",
    "        #if this is the first word, sentence vector starts out with single embedding\n",
    "        #if not the first word, add word embedding to previous embeddings as part of cumulative sentence vector\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            \n",
    "            #add 1 to word counter for each iteration\n",
    "            numw+=1\n",
    "        \n",
    "        #if there's an error, do nothing\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #when finished, return the overall sentence vector divided by the number of words \n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "#create empty list in which to store sentence embeddings\n",
    "X = []\n",
    "\n",
    "#for each criterion in overall data list, vectorize the sentence and append to X\n",
    "for sentence in tokenized_final:\n",
    "    X.append(sent_vectorizer(sentence, ft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add embeddings to df and save \n",
    "criteria_final_df['Embedding'] = X\n",
    "criteria_final_df = criteria_final_df.drop(columns = ['Unnamed: 0'])\n",
    "criteria_final_df.to_csv('criteria_final_embedded.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
